{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7153471,"sourceType":"datasetVersion","datasetId":4130659}],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":7774.718192,"end_time":"2023-11-13T09:21:28.081859","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-11-13T07:11:53.363667","version":"2.4.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n!pip install torchgeometry\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":5.358224,"end_time":"2023-11-13T07:12:26.531689","exception":false,"start_time":"2023-11-13T07:12:21.173465","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T05:09:09.620453Z","iopub.execute_input":"2023-12-18T05:09:09.620940Z","iopub.status.idle":"2023-12-18T05:09:22.702053Z","shell.execute_reply.started":"2023-12-18T05:09:09.620914Z","shell.execute_reply":"2023-12-18T05:09:22.700967Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchgeometry\n  Downloading torchgeometry-0.1.2-py2.py3-none-any.whl (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from torchgeometry) (2.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->torchgeometry) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->torchgeometry) (1.3.0)\nInstalling collected packages: torchgeometry\nSuccessfully installed torchgeometry-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip show torch\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T05:09:22.704273Z","iopub.execute_input":"2023-12-18T05:09:22.704923Z","iopub.status.idle":"2023-12-18T05:09:33.952051Z","shell.execute_reply.started":"2023-12-18T05:09:22.704884Z","shell.execute_reply":"2023-12-18T05:09:33.950742Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Name: torch\nVersion: 2.0.0\nSummary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\nHome-page: https://pytorch.org/\nAuthor: PyTorch Team\nAuthor-email: packages@pytorch.org\nLicense: BSD-3\nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: filelock, jinja2, networkx, sympy, typing-extensions\nRequired-by: accelerate, catalyst, easyocr, fastai, kornia, pytorch-ignite, pytorch-lightning, stable-baselines3, timm, torchaudio, torchdata, torchgeometry, torchmetrics, torchtext, torchvision\n","output_type":"stream"}]},{"cell_type":"code","source":"\nfrom torchgeometry.losses import one_hot\nimport os\nimport os.path as osp\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport cv2\nimport time\nimport imageio\nimport matplotlib.pyplot as plt\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch import Tensor\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision.transforms import Resize, PILToTensor, ToPILImage, Compose, InterpolationMode\nfrom collections import OrderedDict\nimport wandb\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport glob\nimport multiprocessing.pool as mpp\nimport multiprocessing as mp\nimport argparse\nimport torch\nimport albumentations as albu\nfrom torchvision.transforms import (Pad, ColorJitter, Resize, FiveCrop, RandomResizedCrop,\n                                    RandomHorizontalFlip, RandomRotation, RandomVerticalFlip)\nimport random\nimport math\nimport numbers\nfrom PIL import Image, ImageOps, ImageEnhance\nimport numpy as np\nimport random\nfrom scipy.ndimage.morphology import generate_binary_structure, binary_erosion\nfrom scipy.ndimage import maximum_filter\nfrom tqdm import tqdm\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport os\nimport torch\nfrom torch import nn\nimport cv2\nimport numpy as np\nimport argparse\nfrom pathlib import Path\n\nfrom pytorch_lightning.loggers import CSVLogger\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-12-18T05:09:33.953736Z","iopub.execute_input":"2023-12-18T05:09:33.954186Z","iopub.status.idle":"2023-12-18T05:09:42.968517Z","shell.execute_reply.started":"2023-12-18T05:09:33.954144Z","shell.execute_reply":"2023-12-18T05:09:42.967635Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"papermill":{"duration":0.082489,"end_time":"2023-11-13T07:12:27.60011","exception":false,"start_time":"2023-11-13T07:12:27.517621","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T05:09:42.970674Z","iopub.execute_input":"2023-12-18T05:09:42.971179Z","iopub.status.idle":"2023-12-18T05:09:42.987004Z","shell.execute_reply.started":"2023-12-18T05:09:42.971146Z","shell.execute_reply":"2023-12-18T05:09:42.986146Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Parameters","metadata":{"papermill":{"duration":0.013953,"end_time":"2023-11-13T07:12:27.627781","exception":false,"start_time":"2023-11-13T07:12:27.613828","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nnum_classes = 6\n\n# Number of epoch\nepochs = 10\n\n# Hyperparameters for training \nlearning_rate = 8e-04\nbatch_size = 8\ndisplay_step = 2\n\n# Model path\ncheckpoint_path = '/kaggle/working/unet_model.pth'\npretrained_path = \"/kaggle/input/unet-checkpoint/unet_model.pth\"\n# Initialize lists to keep track of loss and accuracy\nloss_epoch_array = []\ntrain_accuracy = []\ntest_accuracy = []\nvalid_accuracy = []","metadata":{"papermill":{"duration":0.021712,"end_time":"2023-11-13T07:12:27.662876","exception":false,"start_time":"2023-11-13T07:12:27.641164","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T05:09:42.988258Z","iopub.execute_input":"2023-12-18T05:09:42.988546Z","iopub.status.idle":"2023-12-18T05:09:42.993859Z","shell.execute_reply.started":"2023-12-18T05:09:42.988515Z","shell.execute_reply":"2023-12-18T05:09:42.993130Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"SEED = 42\n\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\n\nImSurf = np.array([255, 255, 255])  # label 0\nBuilding = np.array([255, 0, 0]) # label 1\nLowVeg = np.array([255, 255, 0]) # label 2\nTree = np.array([0, 255, 0]) # label 3\nCar = np.array([0, 255, 255]) # label 4\nClutter = np.array([0, 0, 255]) # label 5\nBoundary = np.array([0, 0, 0]) # label 6\nnum_classes = 6\n\n\n\n\n\ndef pv2rgb(mask):\n    h, w = mask.shape[0], mask.shape[1]\n    mask_rgb = np.zeros(shape=(h, w, 3), dtype=np.uint8)\n    mask_convert = mask[np.newaxis, :, :]\n    mask_rgb[np.all(mask_convert == 3, axis=0)] = [0, 255, 0]\n    mask_rgb[np.all(mask_convert == 0, axis=0)] = [255, 255, 255]\n    mask_rgb[np.all(mask_convert == 1, axis=0)] = [255, 0, 0]\n    mask_rgb[np.all(mask_convert == 2, axis=0)] = [255, 255, 0]\n    mask_rgb[np.all(mask_convert == 4, axis=0)] = [0, 204, 255]\n    mask_rgb[np.all(mask_convert == 5, axis=0)] = [0, 0, 255]\n    return mask_rgb\ndef label2rgb(mask):\n    h, w = mask.shape[0], mask.shape[1]\n    mask_rgb = np.zeros(shape=(h, w, 3), dtype=np.uint8)\n    mask_convert = mask[np.newaxis, :, :]\n    mask_rgb[np.all(mask_convert == 3, axis=0)] = [0, 255, 0]\n    mask_rgb[np.all(mask_convert == 0, axis=0)] = [255, 255, 255]\n    mask_rgb[np.all(mask_convert == 1, axis=0)] = [255, 0, 0]\n    mask_rgb[np.all(mask_convert == 2, axis=0)] = [255, 255, 0]\n    mask_rgb[np.all(mask_convert == 4, axis=0)] = [0, 204, 255]\n    mask_rgb[np.all(mask_convert == 5, axis=0)] = [0, 0, 255]\n    return mask_rgb\n\n\n\ndef car_color_replace(mask):\n    mask = cv2.cvtColor(np.array(mask.copy()), cv2.COLOR_RGB2BGR)\n    mask[np.all(mask == [0, 255, 255], axis=-1)] = [0, 204, 255]\n\n    return mask\n\n\ndef rgb_to_2D_label(_label):\n    _label = _label.transpose(2, 0, 1)\n    label_seg = np.zeros(_label.shape[1:], dtype=np.uint8)\n    label_seg[np.all(_label.transpose([1, 2, 0]) == ImSurf, axis=-1)] = 0\n    label_seg[np.all(_label.transpose([1, 2, 0]) == Building, axis=-1)] = 1\n    label_seg[np.all(_label.transpose([1, 2, 0]) == LowVeg, axis=-1)] = 2\n    label_seg[np.all(_label.transpose([1, 2, 0]) == Tree, axis=-1)] = 3\n    label_seg[np.all(_label.transpose([1, 2, 0]) == Car, axis=-1)] = 4\n    label_seg[np.all(_label.transpose([1, 2, 0]) == Clutter, axis=-1)] = 5\n    label_seg[np.all(_label.transpose([1, 2, 0]) == Boundary, axis=-1)] = 6\n    return label_seg\n\n\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-18T05:09:42.995456Z","iopub.execute_input":"2023-12-18T05:09:42.996044Z","iopub.status.idle":"2023-12-18T05:09:43.018022Z","shell.execute_reply.started":"2023-12-18T05:09:42.996018Z","shell.execute_reply":"2023-12-18T05:09:43.017168Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":" # Split Data","metadata":{}},{"cell_type":"markdown","source":" # Dataloader","metadata":{"papermill":{"duration":0.013261,"end_time":"2023-11-13T07:12:27.690633","exception":false,"start_time":"2023-11-13T07:12:27.677372","status":"completed"},"tags":[]}},{"cell_type":"code","source":"CLASSES = ('ImSurf', 'Building', 'LowVeg', 'Tree', 'Car', 'Clutter')\nPALETTE = [[255, 255, 255], [0, 0, 255], [0, 255, 255], [0, 255, 0], [255, 204, 0], [255, 0, 0]]\n\nORIGIN_IMG_SIZE = (256, 256)\nINPUT_IMG_SIZE = (256, 256)\nTEST_IMG_SIZE = (256, 256)\n\ndef get_training_transform():\n    train_transform = [\n        # albu.RandomBrightnessContrast(brightness_limit=0.25, contrast_limit=0.25, p=0.15),\n        # albu.RandomRotate90(p=0.25),\n        albu.Normalize()\n    ]\n    return albu.Compose(train_transform)\n\ndef get_val_transform():\n    val_transform = [\n        albu.Normalize()\n    ]\n    return albu.Compose(val_transform)\n\n\ndef val_aug(img, mask):\n    img, mask = np.array(img), np.array(mask)\n    aug = get_val_transform()(image=img.copy(), mask=mask.copy())\n    img, mask = aug['image'], aug['mask']\n    return img, mask\n\n\nclass PotsdamDataset(Dataset):\n    def __init__(self, data_root='/kaggle/input/Deep_data_segmentation/Split/Potsdam/', mode='val', img_dir='Image', mask_dir='Label/',\n                 img_suffix='.tif', mask_suffix='.png', transform=val_aug, mosaic_ratio=0.0,\n                 img_size=ORIGIN_IMG_SIZE):\n        self.data_root = data_root\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.img_suffix = img_suffix\n        self.mask_suffix = mask_suffix\n        self.transform = transform\n        self.mode = mode\n        self.mosaic_ratio = mosaic_ratio\n        self.img_size = img_size\n        self.img_ids = self.get_img_ids(self.data_root, self.img_dir, self.mask_dir)\n\n    def __getitem__(self, index):\n        p_ratio = random.random()\n        if p_ratio > self.mosaic_ratio or self.mode == 'val' or self.mode == 'test':\n            img, mask = self.load_img_and_mask(index)\n            if self.transform:\n                img, mask = self.transform(img, mask)\n        else:\n            img, mask = self.load_mosaic_img_and_mask(index)\n            if self.transform:\n                img, mask = self.transform(img, mask)\n\n        img = torch.from_numpy(img).permute(2, 0, 1).float()\n        mask = torch.from_numpy(mask).long()\n        img_id = self.img_ids[index]\n        results = dict(img_id=img_id, img=img, gt_semantic_seg=mask)\n        return results\n\n    def __len__(self):\n        return len(self.img_ids)\n\n    def get_img_ids(self, data_root, img_dir, mask_dir):\n        img_filename_list = os.listdir(osp.join(data_root, img_dir))\n        mask_filename_list = os.listdir(osp.join(data_root, mask_dir))\n        assert len(img_filename_list) == len(mask_filename_list)\n        img_ids = [str(id.split('.')[0]) for id in mask_filename_list]\n        return img_ids\n\n    def load_img_and_mask(self, index):\n        img_id = self.img_ids[index]\n        img_name = osp.join(self.data_root, self.img_dir, img_id + self.img_suffix)\n        mask_name = osp.join(self.data_root, self.mask_dir, img_id + self.mask_suffix)\n        img = Image.open(img_name).convert('RGB')\n        mask = Image.open(mask_name).convert('L')\n        return img, mask\n\n    def load_mosaic_img_and_mask(self, index):\n        indexes = [index] + [random.randint(0, len(self.img_ids) - 1) for _ in range(3)]\n        img_a, mask_a = self.load_img_and_mask(indexes[0])\n        img_b, mask_b = self.load_img_and_mask(indexes[1])\n        img_c, mask_c = self.load_img_and_mask(indexes[2])\n        img_d, mask_d = self.load_img_and_mask(indexes[3])\n\n        img_a, mask_a = np.array(img_a), np.array(mask_a)\n        img_b, mask_b = np.array(img_b), np.array(mask_b)\n        img_c, mask_c = np.array(img_c), np.array(mask_c)\n        img_d, mask_d = np.array(img_d), np.array(mask_d)\n\n        w = self.img_size[1]\n        h = self.img_size[0]\n\n        start_x = w // 4\n        strat_y = h // 4\n        # The coordinates of the splice center\n        offset_x = random.randint(start_x, (w - start_x))\n        offset_y = random.randint(strat_y, (h - strat_y))\n\n        crop_size_a = (offset_x, offset_y)\n        crop_size_b = (w - offset_x, offset_y)\n        crop_size_c = (offset_x, h - offset_y)\n        crop_size_d = (w - offset_x, h - offset_y)\n\n        random_crop_a = albu.RandomCrop(width=crop_size_a[0], height=crop_size_a[1])\n        random_crop_b = albu.RandomCrop(width=crop_size_b[0], height=crop_size_b[1])\n        random_crop_c = albu.RandomCrop(width=crop_size_c[0], height=crop_size_c[1])\n        random_crop_d = albu.RandomCrop(width=crop_size_d[0], height=crop_size_d[1])\n\n        croped_a = random_crop_a(image=img_a.copy(), mask=mask_a.copy())\n        croped_b = random_crop_b(image=img_b.copy(), mask=mask_b.copy())\n        croped_c = random_crop_c(image=img_c.copy(), mask=mask_c.copy())\n        croped_d = random_crop_d(image=img_d.copy(), mask=mask_d.copy())\n\n        img_crop_a, mask_crop_a = croped_a['image'], croped_a['mask']\n        img_crop_b, mask_crop_b = croped_b['image'], croped_b['mask']\n        img_crop_c, mask_crop_c = croped_c['image'], croped_c['mask']\n        img_crop_d, mask_crop_d = croped_d['image'], croped_d['mask']\n\n        top = np.concatenate((img_crop_a, img_crop_b), axis=1)\n        bottom = np.concatenate((img_crop_c, img_crop_d), axis=1)\n        img = np.concatenate((top, bottom), axis=0)\n\n        top_mask = np.concatenate((mask_crop_a, mask_crop_b), axis=1)\n        bottom_mask = np.concatenate((mask_crop_c, mask_crop_d), axis=1)\n        mask = np.concatenate((top_mask, bottom_mask), axis=0)\n        mask = np.ascontiguousarray(mask)\n        img = np.ascontiguousarray(img)\n\n        img = Image.fromarray(img)\n        mask = Image.fromarray(mask)\n\n        return img, mask","metadata":{"execution":{"iopub.status.busy":"2023-12-18T05:09:43.019202Z","iopub.execute_input":"2023-12-18T05:09:43.019651Z","iopub.status.idle":"2023-12-18T05:09:43.050406Z","shell.execute_reply.started":"2023-12-18T05:09:43.019625Z","shell.execute_reply":"2023-12-18T05:09:43.049519Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"data_root =\"/kaggle/input/Deep_data_segmentation/Split/Potsdam/Image/top_potsdam_4_12_0_25.tif\"\n\n\nif os.path.exists(data_root):\n    print(f\"The path '{data_root}' exists.\")\nelse:\n    print(f\"The path '{data_root}' does not exist.\")","metadata":{"execution":{"iopub.status.busy":"2023-12-18T05:09:43.051540Z","iopub.execute_input":"2023-12-18T05:09:43.051849Z","iopub.status.idle":"2023-12-18T05:09:43.081680Z","shell.execute_reply.started":"2023-12-18T05:09:43.051813Z","shell.execute_reply":"2023-12-18T05:09:43.080868Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"The path '/kaggle/input/Deep_data_segmentation/Split/Potsdam/Image/top_potsdam_4_12_0_25.tif' exists.\n","output_type":"stream"}]},{"cell_type":"code","source":"\ntrain_dataset = PotsdamDataset(data_root='/kaggle/input/Deep_data_segmentation/Split/Potsdam/', mode='train',\n                               mosaic_ratio=0.25, transform=val_aug)\n\nval_dataset = PotsdamDataset(transform=val_aug)\ntest_dataset = PotsdamDataset(data_root='/kaggle/input/Deep_data_segmentation/Split/Potsdam/Test',\n                              transform=val_aug)\n\ntrain_loader = DataLoader(dataset=train_dataset,\n                          batch_size=batch_size,\n                          num_workers=0,\n                          pin_memory=False,\n                          shuffle=True,\n                          drop_last=True)\n\nval_loader = DataLoader(dataset=val_dataset,\n                        batch_size=batch_size,\n                        num_workers=2,\n                        shuffle=False,\n                        pin_memory=True,\n                        drop_last=False)\n","metadata":{"papermill":{"duration":0.019707,"end_time":"2023-11-13T07:12:27.79702","exception":false,"start_time":"2023-11-13T07:12:27.777313","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T05:09:43.082787Z","iopub.execute_input":"2023-12-18T05:09:43.083133Z","iopub.status.idle":"2023-12-18T05:09:46.886876Z","shell.execute_reply.started":"2023-12-18T05:09:43.083100Z","shell.execute_reply":"2023-12-18T05:09:46.886013Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Metric","metadata":{}},{"cell_type":"code","source":"class Evaluator(object):\n    def __init__(self, num_class):\n        self.num_class = num_class\n        self.confusion_matrix = np.zeros((self.num_class,) * 2)\n        self.eps = 1e-8\n\n    def get_tp_fp_tn_fn(self):\n        tp = np.diag(self.confusion_matrix)\n        fp = self.confusion_matrix.sum(axis=0) - np.diag(self.confusion_matrix)\n        fn = self.confusion_matrix.sum(axis=1) - np.diag(self.confusion_matrix)\n        tn = np.diag(self.confusion_matrix).sum() - np.diag(self.confusion_matrix)\n        return tp, fp, tn, fn\n\n    def Precision(self):\n        tp, fp, tn, fn = self.get_tp_fp_tn_fn()\n        precision = tp / (tp + fp)\n        return precision\n\n    def Recall(self):\n        tp, fp, tn, fn = self.get_tp_fp_tn_fn()\n        recall = tp / (tp + fn)\n        return recall\n\n    def F1(self):\n        tp, fp, tn, fn = self.get_tp_fp_tn_fn()\n        Precision = tp / (tp + fp)\n        Recall = tp / (tp + fn)\n        F1 = (2.0 * Precision * Recall) / (Precision + Recall)\n        return F1\n\n    def OA(self):\n        OA = np.diag(self.confusion_matrix).sum() / (self.confusion_matrix.sum() + self.eps)\n        return OA\n\n    def Intersection_over_Union(self):\n        tp, fp, tn, fn = self.get_tp_fp_tn_fn()\n        IoU = tp / (tp + fn + fp)\n        return IoU\n\n    def Dice(self):\n        tp, fp, tn, fn = self.get_tp_fp_tn_fn()\n        Dice = 2 * tp / ((tp + fp) + (tp + fn))\n        return Dice\n\n    def Pixel_Accuracy_Class(self):\n        #         TP                                  TP+FP\n        Acc = np.diag(self.confusion_matrix) / (self.confusion_matrix.sum(axis=0) + self.eps)\n        return Acc\n\n    def Frequency_Weighted_Intersection_over_Union(self):\n        freq = np.sum(self.confusion_matrix, axis=1) / (np.sum(self.confusion_matrix) + self.eps)\n        iou = self.Intersection_over_Union()\n        FWIoU = (freq[freq > 0] * iou[freq > 0]).sum()\n        return FWIoU\n\n    def _generate_matrix(self, gt_image, pre_image):\n        mask = (gt_image >= 0) & (gt_image < self.num_class)\n        label = self.num_class * gt_image[mask].astype('int') + pre_image[mask]\n        count = np.bincount(label, minlength=self.num_class ** 2)\n        confusion_matrix = count.reshape(self.num_class, self.num_class)\n        return confusion_matrix\n\n    def add_batch(self, gt_image, pre_image):\n        assert gt_image.shape == pre_image.shape, 'pre_image shape {}, gt_image shape {}'.format(pre_image.shape,\n                                                                                                 gt_image.shape)\n        self.confusion_matrix += self._generate_matrix(gt_image, pre_image)\n\n    def reset(self):\n        self.confusion_matrix = np.zeros((self.num_class,) * 2)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T05:09:46.890548Z","iopub.execute_input":"2023-12-18T05:09:46.890877Z","iopub.status.idle":"2023-12-18T05:09:46.908342Z","shell.execute_reply.started":"2023-12-18T05:09:46.890823Z","shell.execute_reply":"2023-12-18T05:09:46.907369Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!pip install einops","metadata":{"execution":{"iopub.status.busy":"2023-12-18T05:09:46.909823Z","iopub.execute_input":"2023-12-18T05:09:46.910193Z","iopub.status.idle":"2023-12-18T05:09:58.611867Z","shell.execute_reply.started":"2023-12-18T05:09:46.910160Z","shell.execute_reply":"2023-12-18T05:09:58.610614Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Collecting einops\n  Obtaining dependency information for einops from https://files.pythonhosted.org/packages/29/0b/2d1c0ebfd092e25935b86509a9a817159212d82aa43d7fb07eca4eeff2c2/einops-0.7.0-py3-none-any.whl.metadata\n  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\nDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.7.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model","metadata":{"papermill":{"duration":0.013405,"end_time":"2023-11-13T07:12:28.293069","exception":false,"start_time":"2023-11-13T07:12:28.279664","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**Encoder Block**","metadata":{"papermill":{"duration":0.013187,"end_time":"2023-11-13T07:12:28.319611","exception":false,"start_time":"2023-11-13T07:12:28.306424","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\n\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nimport timm\n\n\nclass ConvBNReLU(nn.Sequential):\n    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, stride=1, norm_layer=nn.BatchNorm2d, bias=False):\n        super(ConvBNReLU, self).__init__(\n            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=bias,\n                      dilation=dilation, stride=stride, padding=((stride - 1) + dilation * (kernel_size - 1)) // 2),\n            norm_layer(out_channels),\n            nn.ReLU6()\n        )\n\n\nclass ConvBN(nn.Sequential):\n    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, stride=1, norm_layer=nn.BatchNorm2d, bias=False):\n        super(ConvBN, self).__init__(\n            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=bias,\n                      dilation=dilation, stride=stride, padding=((stride - 1) + dilation * (kernel_size - 1)) // 2),\n            norm_layer(out_channels)\n        )\n\n\nclass Conv(nn.Sequential):\n    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, stride=1, bias=False):\n        super(Conv, self).__init__(\n            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=bias,\n                      dilation=dilation, stride=stride, padding=((stride - 1) + dilation * (kernel_size - 1)) // 2)\n        )\n\n\nclass SeparableConvBNReLU(nn.Sequential):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1,\n                 norm_layer=nn.BatchNorm2d):\n        super(SeparableConvBNReLU, self).__init__(\n            nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, dilation=dilation,\n                      padding=((stride - 1) + dilation * (kernel_size - 1)) // 2,\n                      groups=in_channels, bias=False),\n            norm_layer(out_channels),\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n            nn.ReLU6()\n        )\n\n\nclass SeparableConvBN(nn.Sequential):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1,\n                 norm_layer=nn.BatchNorm2d):\n        super(SeparableConvBN, self).__init__(\n            nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, dilation=dilation,\n                      padding=((stride - 1) + dilation * (kernel_size - 1)) // 2,\n                      groups=in_channels, bias=False),\n            norm_layer(out_channels),\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        )\n\n\nclass SeparableConv(nn.Sequential):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1):\n        super(SeparableConv, self).__init__(\n            nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, dilation=dilation,\n                      padding=((stride - 1) + dilation * (kernel_size - 1)) // 2,\n                      groups=in_channels, bias=False),\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        )\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.ReLU6, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Conv2d(in_features, hidden_features, 1, 1, 0, bias=True)\n        self.act = act_layer()\n        self.fc2 = nn.Conv2d(hidden_features, out_features, 1, 1, 0, bias=True)\n        self.drop = nn.Dropout(drop, inplace=True)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass GlobalLocalAttention(nn.Module):\n    def __init__(self,\n                 dim=256,\n                 num_heads=16,\n                 qkv_bias=False,\n                 window_size=8,\n                 relative_pos_embedding=True\n                 ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // self.num_heads\n        self.scale = head_dim ** -0.5\n        self.ws = window_size\n\n        self.qkv = Conv(dim, 3*dim, kernel_size=1, bias=qkv_bias)\n        self.local1 = ConvBN(dim, dim, kernel_size=3)\n        self.local2 = ConvBN(dim, dim, kernel_size=1)\n        self.proj = SeparableConvBN(dim, dim, kernel_size=window_size)\n\n        self.attn_x = nn.AvgPool2d(kernel_size=(window_size, 1), stride=1,  padding=(window_size//2 - 1, 0))\n        self.attn_y = nn.AvgPool2d(kernel_size=(1, window_size), stride=1, padding=(0, window_size//2 - 1))\n\n        self.relative_pos_embedding = relative_pos_embedding\n\n        if self.relative_pos_embedding:\n            # define a parameter table of relative position bias\n            self.relative_position_bias_table = nn.Parameter(\n                torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(self.ws)\n            coords_w = torch.arange(self.ws)\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += self.ws - 1  # shift to start from 0\n            relative_coords[:, :, 1] += self.ws - 1\n            relative_coords[:, :, 0] *= 2 * self.ws - 1\n            relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n\n            trunc_normal_(self.relative_position_bias_table, std=.02)\n\n    def pad(self, x, ps):\n        _, _, H, W = x.size()\n        if W % ps != 0:\n            x = F.pad(x, (0, ps - W % ps), mode='reflect')\n        if H % ps != 0:\n            x = F.pad(x, (0, 0, 0, ps - H % ps), mode='reflect')\n        return x\n\n    def pad_out(self, x):\n        x = F.pad(x, pad=(0, 1, 0, 1), mode='reflect')\n        return x\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n\n        local = self.local2(x) + self.local1(x)\n\n        x = self.pad(x, self.ws)\n        B, C, Hp, Wp = x.shape\n        qkv = self.qkv(x)\n\n        q, k, v = rearrange(qkv, 'b (qkv h d) (hh ws1) (ww ws2) -> qkv (b hh ww) h (ws1 ws2) d', h=self.num_heads,\n                            d=C//self.num_heads, hh=Hp//self.ws, ww=Wp//self.ws, qkv=3, ws1=self.ws, ws2=self.ws)\n\n        dots = (q @ k.transpose(-2, -1)) * self.scale\n\n        if self.relative_pos_embedding:\n            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                self.ws * self.ws, self.ws * self.ws, -1)  # Wh*Ww,Wh*Ww,nH\n            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n            dots += relative_position_bias.unsqueeze(0)\n\n        attn = dots.softmax(dim=-1)\n        attn = attn @ v\n\n        attn = rearrange(attn, '(b hh ww) h (ws1 ws2) d -> b (h d) (hh ws1) (ww ws2)', h=self.num_heads,\n                         d=C//self.num_heads, hh=Hp//self.ws, ww=Wp//self.ws, ws1=self.ws, ws2=self.ws)\n\n        attn = attn[:, :, :H, :W]\n\n        out = self.attn_x(F.pad(attn, pad=(0, 0, 0, 1), mode='reflect')) + \\\n              self.attn_y(F.pad(attn, pad=(0, 1, 0, 0), mode='reflect'))\n\n        out = out + local\n        out = self.pad_out(out)\n        out = self.proj(out)\n        # print(out.size())\n        out = out[:, :, :H, :W]\n\n        return out\n\n\nclass Block(nn.Module):\n    def __init__(self, dim=256, num_heads=16,  mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.ReLU6, norm_layer=nn.BatchNorm2d, window_size=8):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = GlobalLocalAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, window_size=window_size)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, out_features=dim, act_layer=act_layer, drop=drop)\n        self.norm2 = norm_layer(dim)\n\n    def forward(self, x):\n\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\n\n\nclass WF(nn.Module):\n    def __init__(self, in_channels=128, decode_channels=128, eps=1e-8):\n        super(WF, self).__init__()\n        self.pre_conv = Conv(in_channels, decode_channels, kernel_size=1)\n\n        self.weights = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)\n        self.eps = eps\n        self.post_conv = ConvBNReLU(decode_channels, decode_channels, kernel_size=3)\n\n    def forward(self, x, res):\n        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n        weights = nn.ReLU()(self.weights)\n        fuse_weights = weights / (torch.sum(weights, dim=0) + self.eps)\n        x = fuse_weights[0] * self.pre_conv(res) + fuse_weights[1] * x\n        x = self.post_conv(x)\n        return x\n\n\nclass FeatureRefinementHead(nn.Module):\n    def __init__(self, in_channels=64, decode_channels=64):\n        super().__init__()\n        self.pre_conv = Conv(in_channels, decode_channels, kernel_size=1)\n\n        self.weights = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)\n        self.eps = 1e-8\n        self.post_conv = ConvBNReLU(decode_channels, decode_channels, kernel_size=3)\n\n        self.pa = nn.Sequential(nn.Conv2d(decode_channels, decode_channels, kernel_size=3, padding=1, groups=decode_channels),\n                                nn.Sigmoid())\n        self.ca = nn.Sequential(nn.AdaptiveAvgPool2d(1),\n                                Conv(decode_channels, decode_channels//16, kernel_size=1),\n                                nn.ReLU6(),\n                                Conv(decode_channels//16, decode_channels, kernel_size=1),\n                                nn.Sigmoid())\n\n        self.shortcut = ConvBN(decode_channels, decode_channels, kernel_size=1)\n        self.proj = SeparableConvBN(decode_channels, decode_channels, kernel_size=3)\n        self.act = nn.ReLU6()\n\n    def forward(self, x, res):\n        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n        weights = nn.ReLU()(self.weights)\n        fuse_weights = weights / (torch.sum(weights, dim=0) + self.eps)\n        x = fuse_weights[0] * self.pre_conv(res) + fuse_weights[1] * x\n        x = self.post_conv(x)\n        shortcut = self.shortcut(x)\n        pa = self.pa(x) * x\n        ca = self.ca(x) * x\n        x = pa + ca\n        x = self.proj(x) + shortcut\n        x = self.act(x)\n\n        return x\n\n\nclass AuxHead(nn.Module):\n\n    def __init__(self, in_channels=64, num_classes=8):\n        super().__init__()\n        self.conv = ConvBNReLU(in_channels, in_channels)\n        self.drop = nn.Dropout(0.1)\n        self.conv_out = Conv(in_channels, num_classes, kernel_size=1)\n\n    def forward(self, x, h, w):\n        feat = self.conv(x)\n        feat = self.drop(feat)\n        feat = self.conv_out(feat)\n        feat = F.interpolate(feat, size=(h, w), mode='bilinear', align_corners=False)\n        return feat\n\n\nclass Decoder(nn.Module):\n    def __init__(self,\n                 encoder_channels=(64, 128, 256, 512),\n                 decode_channels=64,\n                 dropout=0.1,\n                 window_size=8,\n                 num_classes=6):\n        super(Decoder, self).__init__()\n\n        self.pre_conv = ConvBN(encoder_channels[-1], decode_channels, kernel_size=1)\n        self.b4 = Block(dim=decode_channels, num_heads=8, window_size=window_size)\n\n        self.b3 = Block(dim=decode_channels, num_heads=8, window_size=window_size)\n        self.p3 = WF(encoder_channels[-2], decode_channels)\n\n        self.b2 = Block(dim=decode_channels, num_heads=8, window_size=window_size)\n        self.p2 = WF(encoder_channels[-3], decode_channels)\n\n        if self.training:\n            self.up4 = nn.UpsamplingBilinear2d(scale_factor=4)\n            self.up3 = nn.UpsamplingBilinear2d(scale_factor=2)\n            self.aux_head = AuxHead(decode_channels, num_classes)\n\n        self.p1 = FeatureRefinementHead(encoder_channels[-4], decode_channels)\n\n        self.segmentation_head = nn.Sequential(ConvBNReLU(decode_channels, decode_channels),\n                                               nn.Dropout2d(p=dropout, inplace=True),\n                                               Conv(decode_channels, num_classes, kernel_size=1))\n        self.init_weight()\n\n    def forward(self, res1, res2, res3, res4, h, w):\n        if self.training:\n            x = self.b4(self.pre_conv(res4))\n            h4 = self.up4(x)\n\n            x = self.p3(x, res3)\n            x = self.b3(x)\n            h3 = self.up3(x)\n\n            x = self.p2(x, res2)\n            x = self.b2(x)\n            h2 = x\n            x = self.p1(x, res1)\n            x = self.segmentation_head(x)\n            x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=False)\n\n            ah = h4 + h3 + h2\n            ah = self.aux_head(ah, h, w)\n\n            return x, ah\n        else:\n            x = self.b4(self.pre_conv(res4))\n            x = self.p3(x, res3)\n            x = self.b3(x)\n\n            x = self.p2(x, res2)\n            x = self.b2(x)\n\n            x = self.p1(x, res1)\n\n            x = self.segmentation_head(x)\n            x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=False)\n\n            return x\n\n    def init_weight(self):\n        for m in self.children():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, a=1)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n\nclass UNetFormer(nn.Module):\n    def __init__(self,\n                 decode_channels=64,\n                 dropout=0.1,\n                 backbone_name='swsl_resnet18',\n                 pretrained=True,\n                 window_size=8,\n                 num_classes=6\n                 ):\n        super().__init__()\n\n        self.backbone = timm.create_model(backbone_name, features_only=True, output_stride=32,\n                                          out_indices=(1, 2, 3, 4), pretrained=pretrained)\n        encoder_channels = self.backbone.feature_info.channels()\n\n        self.decoder = Decoder(encoder_channels, decode_channels, dropout, window_size, num_classes)\n\n    def forward(self, x):\n        h, w = x.size()[-2:]\n        res1, res2, res3, res4 = self.backbone(x)\n        if self.training:\n            x, ah = self.decoder(res1, res2, res3, res4, h, w)\n            return x, ah\n        else:\n            x = self.decoder(res1, res2, res3, res4, h, w)\n            return x","metadata":{"papermill":{"duration":0.024183,"end_time":"2023-11-13T07:12:28.357222","exception":false,"start_time":"2023-11-13T07:12:28.333039","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T05:09:58.613653Z","iopub.execute_input":"2023-12-18T05:09:58.613992Z","iopub.status.idle":"2023-12-18T05:09:59.039749Z","shell.execute_reply.started":"2023-12-18T05:09:58.613964Z","shell.execute_reply":"2023-12-18T05:09:59.038759Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**Decoder block**","metadata":{"papermill":{"duration":0.013371,"end_time":"2023-11-13T07:12:28.384103","exception":false,"start_time":"2023-11-13T07:12:28.370732","status":"completed"},"tags":[]}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.024252,"end_time":"2023-11-13T07:12:28.4219","exception":false,"start_time":"2023-11-13T07:12:28.397648","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Bottle neck**","metadata":{"papermill":{"duration":0.013422,"end_time":"2023-11-13T07:12:28.448754","exception":false,"start_time":"2023-11-13T07:12:28.435332","status":"completed"},"tags":[]}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.023287,"end_time":"2023-11-13T07:12:28.485664","exception":false,"start_time":"2023-11-13T07:12:28.462377","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Unet model**","metadata":{"papermill":{"duration":0.013331,"end_time":"2023-11-13T07:12:28.512584","exception":false,"start_time":"2023-11-13T07:12:28.499253","status":"completed"},"tags":[]}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.026209,"end_time":"2023-11-13T07:12:28.552346","exception":false,"start_time":"2023-11-13T07:12:28.526137","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# Instantiate your UNet model\nmodel = UNetFormer()\n\n\n# Print the shape of the outputmodel = UNet(n_class=6)\n\n# Create a dummy batch with random images\ndummy_batch_size = 16  # Choose your batch size\ndummy_input_batch = torch.randn(dummy_batch_size, 3, 256, 256)  # Batch size, 3 channels, 256x256\n\n# Forward pass\noutput_batch, _ = model(dummy_input_batch)\n\n# Print the shape of the output batch\n\nprint(\"Output batch shape:\", output_batch.shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T05:09:59.040865Z","iopub.execute_input":"2023-12-18T05:09:59.041132Z","iopub.status.idle":"2023-12-18T05:10:01.965422Z","shell.execute_reply.started":"2023-12-18T05:09:59.041109Z","shell.execute_reply":"2023-12-18T05:10:01.964469Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name swsl_resnet18 to current resnet18.fb_swsl_ig1b_ft_in1k.\n  model = create_fn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3ac5881695a437093f0eb51886f1955"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3483.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","output_type":"stream"},{"name":"stdout","text":"Output batch shape: torch.Size([16, 6, 256, 256])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Loss function","metadata":{"papermill":{"duration":0.013249,"end_time":"2023-11-13T07:12:28.579095","exception":false,"start_time":"2023-11-13T07:12:28.565846","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def label_smoothed_nll_loss(\n    lprobs: torch.Tensor, target: torch.Tensor, epsilon: float, ignore_index=None, reduction=\"mean\", dim=-1\n) -> torch.Tensor:\n    \"\"\"\n\n    Source: https://github.com/pytorch/fairseq/blob/master/fairseq/criterions/label_smoothed_cross_entropy.py\n\n    :param lprobs: Log-probabilities of predictions (e.g after log_softmax)\n    :param target:\n    :param epsilon:\n    :param ignore_index:\n    :param reduction:\n    :return:\n    \"\"\"\n    if target.dim() == lprobs.dim() - 1:\n        target = target.unsqueeze(dim)\n\n    if ignore_index is not None:\n        pad_mask = target.eq(ignore_index)\n        target = target.masked_fill(pad_mask, 0)\n        nll_loss = -lprobs.gather(dim=dim, index=target)\n        smooth_loss = -lprobs.sum(dim=dim, keepdim=True)\n\n        # nll_loss.masked_fill_(pad_mask, 0.0)\n        # smooth_loss.masked_fill_(pad_mask, 0.0)\n        nll_loss = nll_loss.masked_fill(pad_mask, 0.0)\n        smooth_loss = smooth_loss.masked_fill(pad_mask, 0.0)\n    else:\n        nll_loss = -lprobs.gather(dim=dim, index=target)\n        smooth_loss = -lprobs.sum(dim=dim, keepdim=True)\n\n        nll_loss = nll_loss.squeeze(dim)\n        smooth_loss = smooth_loss.squeeze(dim)\n\n    if reduction == \"sum\":\n        nll_loss = nll_loss.sum()\n        smooth_loss = smooth_loss.sum()\n    if reduction == \"mean\":\n        nll_loss = nll_loss.mean()\n        smooth_loss = smooth_loss.mean()\n\n    eps_i = epsilon / lprobs.size(dim)\n    loss = (1.0 - epsilon) * nll_loss + eps_i * smooth_loss\n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-12-18T05:10:01.969428Z","iopub.execute_input":"2023-12-18T05:10:01.969793Z","iopub.status.idle":"2023-12-18T05:10:01.982538Z","shell.execute_reply.started":"2023-12-18T05:10:01.969761Z","shell.execute_reply":"2023-12-18T05:10:01.981546Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"__all__ = [\"SoftCrossEntropyLoss\"]\nfrom typing import Optional\nfrom torch.nn.modules.loss import _Loss\nfrom typing import List\nclass SoftCrossEntropyLoss(nn.Module):\n    \"\"\"\n    Drop-in replacement for nn.CrossEntropyLoss with few additions:\n    - Support of label smoothing\n    \"\"\"\n\n    __constants__ = [\"reduction\", \"ignore_index\", \"smooth_factor\"]\n\n    def __init__(self, reduction: str = \"mean\", smooth_factor: float = 0.0, ignore_index: Optional[int] = -100, dim=1):\n        super().__init__()\n        self.smooth_factor = smooth_factor\n        self.ignore_index = ignore_index\n        self.reduction = reduction\n        self.dim = dim\n\n    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n        log_prob = F.log_softmax(input, dim=self.dim)\n        pad_mask = target.eq(self.ignore_index)\n        target = target.masked_fill(pad_mask, 0)\n        log_prob = log_prob.masked_fill(pad_mask.unsqueeze(1), 0)\n        return label_smoothed_nll_loss(\n            log_prob,\n            target,\n            epsilon=self.smooth_factor,\n            ignore_index=self.ignore_index,\n            reduction=self.reduction,\n            dim=self.dim,\n        )\n__all__ = [\"JointLoss\", \"WeightedLoss\"]\n\n\nclass WeightedLoss(_Loss):\n    \"\"\"Wrapper class around loss function that applies weighted with fixed factor.\n    This class helps to balance multiple losses if they have different scales\n    \"\"\"\n\n    def __init__(self, loss, weight=1.0):\n        super().__init__()\n        self.loss = loss\n        self.weight = weight\n\n    def forward(self, *input):\n        return self.loss(*input) * self.weight\nclass JointLoss(_Loss):\n    \"\"\"\n    Wrap two loss functions into one. This class computes a weighted sum of two losses.\n    \"\"\"\n\n    def __init__(self, first: nn.Module, second: nn.Module, first_weight=1.0, second_weight=1.0):\n        super().__init__()\n        self.first = WeightedLoss(first, first_weight)\n        self.second = WeightedLoss(second, second_weight)\n\n    def forward(self, *input):\n        return self.first(*input) + self.second(*input)\n__all__ = [\"DiceLoss\"]\n\nBINARY_MODE = \"binary\"\nMULTICLASS_MODE = \"multiclass\"\nMULTILABEL_MODE = \"multilabel\"\ndef soft_dice_score(\n    output: torch.Tensor, target: torch.Tensor, smooth: float = 0.0, eps: float = 1e-7, dims=None\n) -> torch.Tensor:\n    \"\"\"\n\n    :param output:\n    :param target:\n    :param smooth:\n    :param eps:\n    :return:\n\n    Shape:\n        - Input: :math:`(N, NC, *)` where :math:`*` means any number\n            of additional dimensions\n        - Target: :math:`(N, NC, *)`, same shape as the input\n        - Output: scalar.\n\n    \"\"\"\n    assert output.size() == target.size()\n    if dims is not None:\n        intersection = torch.sum(output * target, dim=dims)\n        cardinality = torch.sum(output + target, dim=dims)\n    else:\n        intersection = torch.sum(output * target)\n        cardinality = torch.sum(output + target)\n    dice_score = (2.0 * intersection + smooth) / (cardinality + smooth).clamp_min(eps)\n    return dice_score\nclass DiceLoss(_Loss):\n    \"\"\"\n    Implementation of Dice loss for image segmentation task.\n    It supports binary, multiclass and multilabel cases\n    \"\"\"\n\n    def __init__(\n        self,\n        mode: str = 'multiclass',\n        classes: List[int] = None,\n        log_loss=False,\n        from_logits=True,\n        smooth: float = 0.0,\n        ignore_index=None,\n        eps=1e-7,\n    ):\n        \"\"\"\n\n        :param mode: Metric mode {'binary', 'multiclass', 'multilabel'}\n        :param classes: Optional list of classes that contribute in loss computation;\n        By default, all channels are included.\n        :param log_loss: If True, loss computed as `-log(jaccard)`; otherwise `1 - jaccard`\n        :param from_logits: If True assumes input is raw logits\n        :param smooth:\n        :param ignore_index: Label that indicates ignored pixels (does not contribute to loss)\n        :param eps: Small epsilon for numerical stability\n        \"\"\"\n        assert mode in {BINARY_MODE, MULTILABEL_MODE, MULTICLASS_MODE}\n        super(DiceLoss, self).__init__()\n        self.mode = mode\n        if classes is not None:\n            assert mode != BINARY_MODE, \"Masking classes is not supported with mode=binary\"\n            classes = to_tensor(classes, dtype=torch.long)\n\n        self.classes = classes\n        self.from_logits = from_logits\n        self.smooth = smooth\n        self.eps = eps\n        self.ignore_index = ignore_index\n        self.log_loss = log_loss\n\n    def forward(self, y_pred: Tensor, y_true: Tensor) -> Tensor:\n        \"\"\"\n\n        :param y_pred: NxCxHxW\n        :param y_true: NxHxW\n        :return: scalar\n        \"\"\"\n        assert y_true.size(0) == y_pred.size(0)\n\n        if self.from_logits:\n            # Apply activations to get [0..1] class probabilities\n            # Using Log-Exp as this gives more numerically stable result and does not cause vanishing gradient on\n            # extreme values 0 and 1\n            if self.mode == MULTICLASS_MODE:\n                y_pred = y_pred.log_softmax(dim=1).exp()\n            else:\n                y_pred = F.logsigmoid(y_pred).exp()\n\n        bs = y_true.size(0)\n        num_classes = y_pred.size(1)\n        dims = (0, 2)\n\n        if self.mode == BINARY_MODE:\n            y_true = y_true.view(bs, 1, -1)\n            y_pred = y_pred.view(bs, 1, -1)\n\n            if self.ignore_index is not None:\n                mask = y_true != self.ignore_index\n                y_pred = y_pred * mask\n                y_true = y_true * mask\n\n        if self.mode == MULTICLASS_MODE:\n            y_true = y_true.view(bs, -1)\n            y_pred = y_pred.view(bs, num_classes, -1)\n\n            if self.ignore_index is not None:\n                mask = y_true != self.ignore_index\n                y_pred = y_pred * mask.unsqueeze(1)\n\n                y_true = F.one_hot((y_true * mask).to(torch.long), num_classes)  # N,H*W -> N,H*W, C\n                y_true = y_true.permute(0, 2, 1) * mask.unsqueeze(1)  # H, C, H*W\n            else:\n                y_true = F.one_hot(y_true, num_classes)  # N,H*W -> N,H*W, C\n                y_true = y_true.permute(0, 2, 1)  # H, C, H*W\n\n        if self.mode == MULTILABEL_MODE:\n            y_true = y_true.view(bs, num_classes, -1)\n            y_pred = y_pred.view(bs, num_classes, -1)\n\n            if self.ignore_index is not None:\n                mask = y_true != self.ignore_index\n                y_pred = y_pred * mask\n                y_true = y_true * mask\n\n        scores = soft_dice_score(y_pred, y_true.type_as(y_pred), smooth=self.smooth, eps=self.eps, dims=dims)\n\n        if self.log_loss:\n            loss = -torch.log(scores.clamp_min(self.eps))\n        else:\n            loss = 1.0 - scores\n\n        # Dice loss is undefined for non-empty classes\n        # So we zero contribution of channel that does not have true pixels\n        # NOTE: A better workaround would be to use loss term `mean(y_pred)`\n        # for this case, however it will be a modified jaccard loss\n\n        mask = y_true.sum(dims) > 0\n        loss *= mask.to(loss.dtype)\n\n        if self.classes is not None:\n            loss = loss[self.classes]\n\n        return loss.mean()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T05:10:01.984103Z","iopub.execute_input":"2023-12-18T05:10:01.984418Z","iopub.status.idle":"2023-12-18T05:10:02.024160Z","shell.execute_reply.started":"2023-12-18T05:10:01.984390Z","shell.execute_reply":"2023-12-18T05:10:02.023172Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"\nclass UnetLoss(nn.Module):\n    def __init__(self, ignore_index=255):\n        super().__init__()\n        self.main_loss = JointLoss(SoftCrossEntropyLoss(smooth_factor=0.05, ignore_index=ignore_index),\n                                   DiceLoss(smooth=0.05, ignore_index=ignore_index), 1.0, 1.0)\n        self.aux_loss = SoftCrossEntropyLoss(smooth_factor=0.05, ignore_index=ignore_index)\n\n    def forward(self, logits, labels):\n        if self.training and len(logits) == 2:\n            logit_main, logit_aux = logits\n            loss = self.main_loss(logit_main, labels) + 0.4 * self.aux_loss(logit_aux, labels)\n        else:\n            loss = self.main_loss(logits, labels)\n\n        return loss","metadata":{"papermill":{"duration":0.027653,"end_time":"2023-11-13T07:12:28.620311","exception":false,"start_time":"2023-11-13T07:12:28.592658","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T05:10:02.025615Z","iopub.execute_input":"2023-12-18T05:10:02.025953Z","iopub.status.idle":"2023-12-18T05:10:02.033386Z","shell.execute_reply.started":"2023-12-18T05:10:02.025927Z","shell.execute_reply":"2023-12-18T05:10:02.032486Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"papermill":{"duration":0.013344,"end_time":"2023-11-13T07:12:28.647307","exception":false,"start_time":"2023-11-13T07:12:28.633963","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**Initialize weights**","metadata":{"papermill":{"duration":0.014389,"end_time":"2023-11-13T07:12:28.675129","exception":false,"start_time":"2023-11-13T07:12:28.66074","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def weights_init(model):\n    if isinstance(model, nn.Linear):\n        # Xavier Distribution\n        torch.nn.init.xavier_uniform_(model.weight)","metadata":{"papermill":{"duration":0.020816,"end_time":"2023-11-13T07:12:28.709398","exception":false,"start_time":"2023-11-13T07:12:28.688582","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T05:10:02.034418Z","iopub.execute_input":"2023-12-18T05:10:02.034701Z","iopub.status.idle":"2023-12-18T05:10:02.042294Z","shell.execute_reply.started":"2023-12-18T05:10:02.034678Z","shell.execute_reply":"2023-12-18T05:10:02.041390Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def save_model(model, optimizer, path):\n    checkpoint = {\n        \"model\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    torch.save(checkpoint, path)\n\ndef load_model(model, optimizer, path):\n    checkpoint = torch.load(path)\n    model.load_state_dict(checkpoint[\"model\"])\n    optimizer.load_state_dict(checkpoint['optimizer'])\n    return model, optimizer","metadata":{"papermill":{"duration":0.021664,"end_time":"2023-11-13T07:12:28.74459","exception":false,"start_time":"2023-11-13T07:12:28.722926","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T05:10:02.043480Z","iopub.execute_input":"2023-12-18T05:10:02.043814Z","iopub.status.idle":"2023-12-18T05:10:02.052175Z","shell.execute_reply.started":"2023-12-18T05:10:02.043783Z","shell.execute_reply":"2023-12-18T05:10:02.051200Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"**Train model**","metadata":{"papermill":{"duration":0.01333,"end_time":"2023-11-13T07:12:28.771998","exception":false,"start_time":"2023-11-13T07:12:28.758668","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def train(train_dataloader, valid_dataloader, learing_rate_scheduler, epoch, display_step):\n    print(f\"Start epoch #{epoch+1}, learning rate for this epoch: {learing_rate_scheduler.get_last_lr()}\")\n    start_time = time.time()\n    train_loss_epoch = 0\n    test_loss_epoch = 0\n    last_loss = 999999999\n    model.train()\n    metrics_train = Evaluator(num_class=6)\n    metrics_val = Evaluator(num_class=6)\n\n    for i,input in enumerate(tqdm(train_dataloader)):\n        # Load data into GPU\n        data=input['img']\n        masks_true = input['gt_semantic_seg']\n        data,mask = data.to(device),masks_true.to(device)\n        optimizer.zero_grad()\n        prediction, head = model(data)\n        # Backpropagation, compute gradients\n        loss = loss_function(prediction, mask.long())\n        pre_mask = nn.Softmax(dim=1)(prediction)\n        pre_mask = pre_mask.argmax(dim=1)\n        for i in range(mask.shape[0]):\n            metrics_train.add_batch(mask[i].cpu().numpy(), pre_mask[i].cpu().numpy())\n        loss.backward()\n\n        # Apply gradients\n        optimizer.step()\n\n        # Save loss\n        train_loss_epoch += loss.item()\n    print(f\"Done epoch #{epoch+1}, time for this epoch: {time.time()-start_time}s\")\n    train_loss_epoch /= (i + 1)\n    mIoU = np.nanmean(metrics_train.Intersection_over_Union()[:-1])\n    F1 = np.nanmean(metrics_train.F1()[:-1])\n    OA = np.nanmean(metrics_train.OA())\n    iou_per_class = metrics_train.Intersection_over_Union()\n    train_eval_value =  (iou_per_class,mIoU,F1,OA)\n    metrics_train.reset()\n    # Evaluate the validation set\n    model.eval()\n    with torch.no_grad():\n        for input in tqdm(valid_dataloader):\n            data=input['img'].to(device)\n            mask = input['gt_semantic_seg'].to(device)\n            prediction = model(data)\n            test_loss = loss_function(prediction, mask.long())\n            pre_mask = nn.Softmax(dim=1)(prediction)\n            pre_mask = pre_mask.argmax(dim=1)\n            test_loss_epoch += test_loss.item()\n            for i in range(mask.shape[0]):\n                metrics_val.add_batch(mask[i].cpu().numpy(), pre_mask[i].cpu().numpy())\n    test_loss_epoch /= (i + 1)\n    mIoU = np.nanmean(metrics_val.Intersection_over_Union()[:-1])\n    F1 = np.nanmean(metrics_val.F1()[:-1])\n    OA = np.nanmean(metrics_val.OA())\n    iou_per_class_val = metrics_val.Intersection_over_Union()\n    eval_value =  (iou_per_class_val,mIoU,F1,OA)\n    print(eval_value)\n    metrics_val.reset()\n    return train_loss_epoch, train_eval_value, test_loss_epoch, eval_value\n","metadata":{"papermill":{"duration":0.025915,"end_time":"2023-11-13T07:12:28.811424","exception":false,"start_time":"2023-11-13T07:12:28.785509","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T05:10:02.053360Z","iopub.execute_input":"2023-12-18T05:10:02.054014Z","iopub.status.idle":"2023-12-18T05:10:02.068869Z","shell.execute_reply.started":"2023-12-18T05:10:02.053990Z","shell.execute_reply":"2023-12-18T05:10:02.067985Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"**Test model**","metadata":{}},{"cell_type":"code","source":"\ndef test(dataloader):\n    evaluator = Evaluator(num_class=6)\n    evaluator.reset()\n    results = []\n    for input in tqdm(dataloader):\n        # raw_prediction NxCxHxW\n        raw_predictions = model(input['img'].cuda())\n\n        image_ids = input[\"img_id\"]\n        masks_true = input['gt_semantic_seg']\n\n        raw_predictions = nn.Softmax(dim=1)(raw_predictions)\n        predictions = raw_predictions.argmax(dim=1)\n\n        for i in range(raw_predictions.shape[0]):\n            mask = predictions[i].cpu().numpy()\n            evaluator.add_batch(pre_image=mask, gt_image=masks_true[i].cpu().numpy())\n            mask_name = image_ids[i]\n            results.append((mask, str(args.output_path / mask_name), args.rgb))\n        for i in range(raw_predictions.shape[0]):\n            mask = predictions[i].cpu().numpy()\n            evaluator.add_batch(pre_image=mask, gt_image=masks_true[i].cpu().numpy())\n            mask_name = image_ids[i]\n            results.append((mask, str(args.output_path / mask_name), args.rgb))\n    iou_per_class = evaluator.Intersection_over_Union()\n    f1_per_class = evaluator.F1()\n    OA = evaluator.OA()\n    return iou_per_class, f1_per_class, OA","metadata":{"papermill":{"duration":0.021642,"end_time":"2023-11-13T07:12:28.84675","exception":false,"start_time":"2023-11-13T07:12:28.825108","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T05:10:02.069994Z","iopub.execute_input":"2023-12-18T05:10:02.070250Z","iopub.status.idle":"2023-12-18T05:10:02.082867Z","shell.execute_reply.started":"2023-12-18T05:10:02.070228Z","shell.execute_reply":"2023-12-18T05:10:02.081815Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2023-12-18T05:10:02.084213Z","iopub.execute_input":"2023-12-18T05:10:02.084535Z","iopub.status.idle":"2023-12-18T05:10:02.097469Z","shell.execute_reply.started":"2023-12-18T05:10:02.084497Z","shell.execute_reply":"2023-12-18T05:10:02.096572Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"model = UNetFormer()\nmodel.to(device)","metadata":{"papermill":{"duration":4.022563,"end_time":"2023-11-13T07:12:32.882726","exception":false,"start_time":"2023-11-13T07:12:28.860163","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T05:10:02.098523Z","iopub.execute_input":"2023-12-18T05:10:02.098858Z","iopub.status.idle":"2023-12-18T05:10:05.463507Z","shell.execute_reply.started":"2023-12-18T05:10:02.098815Z","shell.execute_reply":"2023-12-18T05:10:05.462252Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name swsl_resnet18 to current resnet18.fb_swsl_ig1b_ft_in1k.\n  model = create_fn(\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"UNetFormer(\n  (backbone): FeatureListNet(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n      )\n    )\n  )\n  (decoder): Decoder(\n    (pre_conv): ConvBN(\n      (0): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (b4): Block(\n      (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (attn): GlobalLocalAttention(\n        (qkv): Conv(\n          (0): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        )\n        (local1): ConvBN(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (local2): ConvBN(\n          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (proj): SeparableConvBN(\n          (0): Conv2d(64, 64, kernel_size=(8, 8), stride=(1, 1), padding=(3, 3), groups=64, bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        )\n        (attn_x): AvgPool2d(kernel_size=(8, 1), stride=1, padding=(3, 0))\n        (attn_y): AvgPool2d(kernel_size=(1, 8), stride=1, padding=(0, 3))\n      )\n      (drop_path): Identity()\n      (mlp): Mlp(\n        (fc1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n        (act): ReLU6()\n        (fc2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n        (drop): Dropout(p=0.0, inplace=True)\n      )\n      (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (b3): Block(\n      (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (attn): GlobalLocalAttention(\n        (qkv): Conv(\n          (0): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        )\n        (local1): ConvBN(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (local2): ConvBN(\n          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (proj): SeparableConvBN(\n          (0): Conv2d(64, 64, kernel_size=(8, 8), stride=(1, 1), padding=(3, 3), groups=64, bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        )\n        (attn_x): AvgPool2d(kernel_size=(8, 1), stride=1, padding=(3, 0))\n        (attn_y): AvgPool2d(kernel_size=(1, 8), stride=1, padding=(0, 3))\n      )\n      (drop_path): Identity()\n      (mlp): Mlp(\n        (fc1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n        (act): ReLU6()\n        (fc2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n        (drop): Dropout(p=0.0, inplace=True)\n      )\n      (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (p3): WF(\n      (pre_conv): Conv(\n        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      )\n      (post_conv): ConvBNReLU(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6()\n      )\n    )\n    (b2): Block(\n      (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (attn): GlobalLocalAttention(\n        (qkv): Conv(\n          (0): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        )\n        (local1): ConvBN(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (local2): ConvBN(\n          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (proj): SeparableConvBN(\n          (0): Conv2d(64, 64, kernel_size=(8, 8), stride=(1, 1), padding=(3, 3), groups=64, bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        )\n        (attn_x): AvgPool2d(kernel_size=(8, 1), stride=1, padding=(3, 0))\n        (attn_y): AvgPool2d(kernel_size=(1, 8), stride=1, padding=(0, 3))\n      )\n      (drop_path): Identity()\n      (mlp): Mlp(\n        (fc1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n        (act): ReLU6()\n        (fc2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n        (drop): Dropout(p=0.0, inplace=True)\n      )\n      (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (p2): WF(\n      (pre_conv): Conv(\n        (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      )\n      (post_conv): ConvBNReLU(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6()\n      )\n    )\n    (up4): UpsamplingBilinear2d(scale_factor=4.0, mode='bilinear')\n    (up3): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')\n    (aux_head): AuxHead(\n      (conv): ConvBNReLU(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6()\n      )\n      (drop): Dropout(p=0.1, inplace=False)\n      (conv_out): Conv(\n        (0): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      )\n    )\n    (p1): FeatureRefinementHead(\n      (pre_conv): Conv(\n        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      )\n      (post_conv): ConvBNReLU(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6()\n      )\n      (pa): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n        (1): Sigmoid()\n      )\n      (ca): Sequential(\n        (0): AdaptiveAvgPool2d(output_size=1)\n        (1): Conv(\n          (0): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        )\n        (2): ReLU6()\n        (3): Conv(\n          (0): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        )\n        (4): Sigmoid()\n      )\n      (shortcut): ConvBN(\n        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (proj): SeparableConvBN(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      )\n      (act): ReLU6()\n    )\n    (segmentation_head): Sequential(\n      (0): ConvBNReLU(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6()\n      )\n      (1): Dropout2d(p=0.1, inplace=True)\n      (2): Conv(\n        (0): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\n# Check if CUDA is available\nif torch.cuda.is_available():\n    # Use GPU-accelerated PyTorch functions here\n    print(\"CUDA is available.\")\nelse:\n    print(\"CUDA is not available.\")","metadata":{"papermill":{"duration":0.014071,"end_time":"2023-11-13T07:12:32.911241","exception":false,"start_time":"2023-11-13T07:12:32.89717","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T05:10:05.464995Z","iopub.execute_input":"2023-12-18T05:10:05.465363Z","iopub.status.idle":"2023-12-18T05:10:05.470761Z","shell.execute_reply.started":"2023-12-18T05:10:05.465330Z","shell.execute_reply":"2023-12-18T05:10:05.469884Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"CUDA is available.\n","output_type":"stream"}]},{"cell_type":"code","source":"\nloss_function = UnetLoss(ignore_index=6)\n# Define the optimizer (Adam optimizer)\noptimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n# optimizer.load_state_dict(checkpoint['optimizer'])\n\n# Learning rate scheduler\nlearing_rate_scheduler = lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.6)","metadata":{"papermill":{"duration":0.023208,"end_time":"2023-11-13T07:12:32.948265","exception":false,"start_time":"2023-11-13T07:12:32.925057","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T05:10:05.472118Z","iopub.execute_input":"2023-12-18T05:10:05.472893Z","iopub.status.idle":"2023-12-18T05:10:05.484237Z","shell.execute_reply.started":"2023-12-18T05:10:05.472859Z","shell.execute_reply":"2023-12-18T05:10:05.483417Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"save_model(model, optimizer, checkpoint_path)\nload_checkpoint_flag=True","metadata":{"papermill":{"duration":0.92582,"end_time":"2023-11-13T07:12:33.887741","exception":false,"start_time":"2023-11-13T07:12:32.961921","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T05:10:05.485175Z","iopub.execute_input":"2023-12-18T05:10:05.485407Z","iopub.status.idle":"2023-12-18T05:10:05.597373Z","shell.execute_reply.started":"2023-12-18T05:10:05.485386Z","shell.execute_reply":"2023-12-18T05:10:05.596591Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# print(\"Model keys:\")\n# print(model.state_dict().keys())\n\n# print(\"\\nCheckpoint keys:\")\n# print(checkpoint['model'].keys())\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T05:10:05.598443Z","iopub.execute_input":"2023-12-18T05:10:05.598725Z","iopub.status.idle":"2023-12-18T05:10:05.602760Z","shell.execute_reply.started":"2023-12-18T05:10:05.598700Z","shell.execute_reply":"2023-12-18T05:10:05.601847Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# # Load the model checkpoint if needed\n# # Load the model checkpoint if needed\n# if load_checkpoint_flag:\n#     checkpoint = torch.load(checkpoint_path)\n#     model, optimize= load_model(model, optimizer, checkpoint)\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T05:10:05.603768Z","iopub.execute_input":"2023-12-18T05:10:05.604060Z","iopub.status.idle":"2023-12-18T05:10:05.611576Z","shell.execute_reply.started":"2023-12-18T05:10:05.604036Z","shell.execute_reply":"2023-12-18T05:10:05.610793Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"wandb.login(\n    # set the wandb project where this run will be logged\n#     project= \"PolypSegment\", \n    key = \"b98d2b806f364f5af900550ec98e26e2f418e8a7\",\n)\nwandb.init(\n    project = \"UnetFormer\"\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T05:10:05.616955Z","iopub.execute_input":"2023-12-18T05:10:05.617439Z","iopub.status.idle":"2023-12-18T05:10:39.426454Z","shell.execute_reply.started":"2023-12-18T05:10:05.617413Z","shell.execute_reply":"2023-12-18T05:10:39.425576Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtrithuy2003\u001b[0m (\u001b[33mdeepbk\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231218_051008-7o8rs371</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/deepbk/UnetFormer/runs/7o8rs371' target=\"_blank\">stellar-haze-10</a></strong> to <a href='https://wandb.ai/deepbk/UnetFormer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/deepbk/UnetFormer' target=\"_blank\">https://wandb.ai/deepbk/UnetFormer</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/deepbk/UnetFormer/runs/7o8rs371' target=\"_blank\">https://wandb.ai/deepbk/UnetFormer/runs/7o8rs371</a>"},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/deepbk/UnetFormer/runs/7o8rs371?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7ac0c560bb20>"},"metadata":{}}]},{"cell_type":"code","source":"\n# Training loop\ntrain_loss_array = []\ntest_loss_array = []\nlast_loss = 9999999999999\nfor epoch in range(30):\n    try:\n        train_loss_epoch = 0\n        test_loss_epoch = 0\n        train_loss_epoch, train_eval_value, test_loss_epoch, eval_value = train(train_loader, \n                                                  val_loader, \n                                                  learing_rate_scheduler, epoch, display_step)\n\n        if test_loss_epoch < last_loss:\n            save_model(model, optimizer, checkpoint_path)\n            last_loss = test_loss_epoch\n\n        iou_value = {}\n        iou_per_class,mIoU,F1,OA=train_eval_value\n        eval_value_train = {'mIoU': mIoU,\n                          'F1': F1,\n                          'OA': OA}                                          \n        print('train:', eval_value_train)\n        train_accuracy.append(OA)\n        wandb.log({'mIoU_train': mIoU,\n                          'F1_train': F1,\n                          'OA_train': OA}) \n\n        for class_name, iou in zip(CLASSES,iou_per_class):\n            wandb.log({f\"{class_name}_train_IOU\": iou})\n            iou_value[class_name] = iou\n        print(iou_value)\n\n        iou_value = {}\n        iou_per_class,mIoU,F1,OA = eval_value\n        eval_value_val = {'mIoU': mIoU,\n                          'F1': F1,\n                          'OA': OA}                                          \n        print('val:', eval_value_val)\n        valid_accuracy.append(OA)\n        wandb.log({'mIoU_val': mIoU,\n                          'F1_val': F1,\n                          'OA_val': OA})\n        for class_name, iou in zip(CLASSES,iou_per_class):\n            wandb.log({f\"{class_name}_val_IoU\": iou})\n            iou_value[class_name] = iou\n        print(iou_value)\n\n        learing_rate_scheduler.step()\n        train_loss_array.append(train_loss_epoch)\n        test_loss_array.append(test_loss_epoch)\n        wandb.log({\"Train loss\": train_loss_epoch, \"Valid loss\": test_loss_epoch})\n        print(\"Epoch {}: loss: {:.4f}, train accuracy: {:.4f}, valid accuracy:{:.4f}\".format(epoch + 1, \n                                            train_loss_array[-1], train_accuracy[-1], valid_accuracy[-1]))\n    except:\n        torch.cuda.empty_cache()\n    ","metadata":{"papermill":{"duration":7726.212208,"end_time":"2023-11-13T09:21:20.114357","exception":false,"start_time":"2023-11-13T07:12:33.902149","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T05:10:39.427907Z","iopub.execute_input":"2023-12-18T05:10:39.428179Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Start epoch #1, learning rate for this epoch: [0.0008]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [09:11<00:00,  3.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #1, time for this epoch: 551.4718897342682s\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1786/1786 [01:09<00:00, 25.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"(array([0.64717044, 0.71061865, 0.57069965, 0.41368214, 0.65190974,\n       0.23593554]), 0.5988161221105496, 0.7435691809186342, 0.7378391393266088)\ntrain: {'mIoU': 0.540594107263203, 'F1': 0.7011514982326859, 'OA': 0.6916848030410895}\n{'ImSurf': 0.5461696314020782, 'Building': 0.5978403874948474, 'LowVeg': 0.5409975433329953, 'Tree': 0.4909470811297462, 'Car': 0.5270158929563479, 'Clutter': 0.12112995597800702}\nval: {'mIoU': 0.5988161221105496, 'F1': 0.7435691809186342, 'OA': 0.7378391393266088}\n{'ImSurf': 0.6471704353350934, 'Building': 0.7106186454794262, 'LowVeg': 0.5706996505027185, 'Tree': 0.4136821409247492, 'Car': 0.6519097383107603, 'Clutter': 0.23593554170604347}\nEpoch 1: loss: 329.2134, train accuracy: 0.6917, valid accuracy:0.7378\nStart epoch #2, learning rate for this epoch: [0.0008]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:33<00:00,  6.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #2, time for this epoch: 273.7788314819336s\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1786/1786 [01:08<00:00, 26.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"(array([0.68695649, 0.78297078, 0.61983229, 0.50404153, 0.71713906,\n       0.29614973]), 0.6621880287955842, 0.7927070636183113, 0.7774214199391732)\ntrain: {'mIoU': 0.6269486900394001, 'F1': 0.7698344353961977, 'OA': 0.7533749128924031}\n{'ImSurf': 0.6221480572775675, 'Building': 0.6988513540495006, 'LowVeg': 0.595572858290271, 'Tree': 0.5726324713810115, 'Car': 0.64553870919865, 'Clutter': 0.21106352881208273}\nval: {'mIoU': 0.6621880287955842, 'F1': 0.7927070636183113, 'OA': 0.7774214199391732}\n{'ImSurf': 0.6869564899064934, 'Building': 0.7829707750703335, 'LowVeg': 0.6198322908245057, 'Tree': 0.5040415322992818, 'Car': 0.7171390558773063, 'Clutter': 0.29614973164879427}\nEpoch 2: loss: 282.3474, train accuracy: 0.7534, valid accuracy:0.7774\nStart epoch #3, learning rate for this epoch: [0.0008]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:34<00:00,  6.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #3, time for this epoch: 274.4078314304352s\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1786/1786 [01:10<00:00, 25.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"(array([0.66596357, 0.74524453, 0.63889073, 0.62319052, 0.66973531,\n       0.32053565]), 0.6686049314960693, 0.8006497670284904, 0.7898071669383785)\ntrain: {'mIoU': 0.6555176310106046, 'F1': 0.7909305541239025, 'OA': 0.7768019336946205}\n{'ImSurf': 0.6527533697869488, 'Building': 0.7395461919631487, 'LowVeg': 0.6202901329673274, 'Tree': 0.6004880609337774, 'Car': 0.6645103994018206, 'Clutter': 0.24660733542412103}\nval: {'mIoU': 0.6686049314960693, 'F1': 0.8006497670284904, 'OA': 0.7898071669383785}\n{'ImSurf': 0.6659635658330025, 'Building': 0.7452445292812538, 'LowVeg': 0.6388907317095047, 'Tree': 0.6231905190495624, 'Car': 0.6697353116070233, 'Clutter': 0.32053564562515835}\nEpoch 3: loss: 265.5068, train accuracy: 0.7768, valid accuracy:0.7898\nStart epoch #4, learning rate for this epoch: [0.0008]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:38<00:00,  6.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #4, time for this epoch: 278.1174740791321s\n","output_type":"stream"},{"name":"stderr","text":" 82%|████████▏ | 1472/1786 [00:56<00:11, 27.08it/s]Exception in thread Thread-12 (_pin_memory_loop):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 51, in _pin_memory_loop\n    do_one_step()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in do_one_step\n    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 307, in rebuild_storage_fd\n    fd = df.detach()\n  File \"/opt/conda/lib/python3.10/multiprocessing/resource_sharer.py\", line 58, in detach\n    return reduction.recv_handle(conn)\n  File \"/opt/conda/lib/python3.10/multiprocessing/reduction.py\", line 189, in recv_handle\n    return recvfds(s, 1)[0]\n  File \"/opt/conda/lib/python3.10/multiprocessing/reduction.py\", line 159, in recvfds\n    raise EOFError\nEOFError\n 83%|████████▎ | 1474/1786 [01:01<00:13, 23.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start epoch #5, learning rate for this epoch: [0.0008]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:35<00:00,  6.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #5, time for this epoch: 275.38521790504456s\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1786/1786 [01:08<00:00, 25.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"(array([0.7124618 , 0.82842149, 0.67119474, 0.65099929, 0.71837127,\n       0.38030143]), 0.7162897166780736, 0.8332444059333295, 0.8243677205651496)\ntrain: {'mIoU': 0.6865965291916514, 'F1': 0.8129960726563905, 'OA': 0.8001853451341474}\n{'ImSurf': 0.6855358480493163, 'Building': 0.7820037733492967, 'LowVeg': 0.6446291964394054, 'Tree': 0.626692057593325, 'Car': 0.6941217705269137, 'Clutter': 0.2996642550868076}\nval: {'mIoU': 0.7162897166780736, 'F1': 0.8332444059333295, 'OA': 0.8243677205651496}\n{'ImSurf': 0.7124617970226633, 'Building': 0.8284214907148536, 'LowVeg': 0.6711947375924444, 'Tree': 0.6509992877394685, 'Car': 0.7183712703209386, 'Clutter': 0.3803014261251818}\nEpoch 5: loss: 245.6623, train accuracy: 0.8002, valid accuracy:0.8244\nStart epoch #6, learning rate for this epoch: [0.00048]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:36<00:00,  6.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #6, time for this epoch: 276.36977338790894s\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 1437/1786 [00:55<00:13, 26.70it/s]Exception in thread Thread-14 (_pin_memory_loop):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 51, in _pin_memory_loop\n    do_one_step()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in do_one_step\n    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 307, in rebuild_storage_fd\n    fd = df.detach()\n  File \"/opt/conda/lib/python3.10/multiprocessing/resource_sharer.py\", line 58, in detach\n    return reduction.recv_handle(conn)\n  File \"/opt/conda/lib/python3.10/multiprocessing/reduction.py\", line 189, in recv_handle\n    return recvfds(s, 1)[0]\n  File \"/opt/conda/lib/python3.10/multiprocessing/reduction.py\", line 159, in recvfds\n    raise EOFError\nEOFError\n 81%|████████  | 1440/1786 [00:55<00:13, 26.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start epoch #7, learning rate for this epoch: [0.00048]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:35<00:00,  6.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #7, time for this epoch: 275.67157340049744s\n","output_type":"stream"},{"name":"stderr","text":" 77%|███████▋  | 1369/1786 [00:51<00:16, 25.30it/s]Exception in thread Thread-15 (_pin_memory_loop):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 51, in _pin_memory_loop\n    do_one_step()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in do_one_step\n    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 307, in rebuild_storage_fd\n    fd = df.detach()\n  File \"/opt/conda/lib/python3.10/multiprocessing/resource_sharer.py\", line 58, in detach\n    return reduction.recv_handle(conn)\n  File \"/opt/conda/lib/python3.10/multiprocessing/reduction.py\", line 189, in recv_handle\n    return recvfds(s, 1)[0]\n  File \"/opt/conda/lib/python3.10/multiprocessing/reduction.py\", line 159, in recvfds\n    raise EOFError\nEOFError\n 77%|███████▋  | 1372/1786 [00:51<00:15, 26.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start epoch #8, learning rate for this epoch: [0.00048]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:37<00:00,  6.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #8, time for this epoch: 277.4517683982849s\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1786/1786 [01:09<00:00, 25.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"(array([0.75350196, 0.87166686, 0.68999635, 0.69742236, 0.75668933,\n       0.45588482]), 0.7538553725901965, 0.8581324360825067, 0.8488821804060251)\ntrain: {'mIoU': 0.7261348851328064, 'F1': 0.8400400618981516, 'OA': 0.8302548363095238}\n{'ImSurf': 0.727848564377963, 'Building': 0.8335586515884625, 'LowVeg': 0.6770692482843818, 'Tree': 0.6689078209028176, 'Car': 0.7232901405104073, 'Clutter': 0.37950579593068245}\nval: {'mIoU': 0.7538553725901965, 'F1': 0.8581324360825067, 'OA': 0.8488821804060251}\n{'ImSurf': 0.753501958324994, 'Building': 0.8716668623317019, 'LowVeg': 0.6899963517000502, 'Tree': 0.6974223564462829, 'Car': 0.756689334147954, 'Clutter': 0.4558848200428075}\nEpoch 8: loss: 221.4385, train accuracy: 0.8303, valid accuracy:0.8489\nStart epoch #9, learning rate for this epoch: [0.00048]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:38<00:00,  6.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #9, time for this epoch: 278.67317819595337s\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1786/1786 [01:09<00:00, 25.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"(array([0.7563142 , 0.87783405, 0.69264493, 0.69789223, 0.75732643,\n       0.45167434]), 0.7564023667075358, 0.8597176962303841, 0.8501161146521159)\ntrain: {'mIoU': 0.7308650771187917, 'F1': 0.8432643422721497, 'OA': 0.8337627710080614}\n{'ImSurf': 0.7321848477977775, 'Building': 0.8363093733797895, 'LowVeg': 0.6831072063765126, 'Tree': 0.6744138279951782, 'Car': 0.7283101300447006, 'Clutter': 0.39520749262693095}\nval: {'mIoU': 0.7564023667075358, 'F1': 0.8597176962303841, 'OA': 0.8501161146521159}\n{'ImSurf': 0.7563142006443715, 'Building': 0.8778340459130101, 'LowVeg': 0.6926449281025869, 'Tree': 0.6978922296110575, 'Car': 0.757326429266653, 'Clutter': 0.45167433945512375}\nEpoch 9: loss: 218.7197, train accuracy: 0.8338, valid accuracy:0.8501\nStart epoch #10, learning rate for this epoch: [0.00048]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:34<00:00,  6.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #10, time for this epoch: 274.7252388000488s\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1786/1786 [01:08<00:00, 26.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"(array([0.77715296, 0.87701959, 0.71892974, 0.71457053, 0.76104244,\n       0.49203452]), 0.7697430544493062, 0.868681393977264, 0.8634297291387707)\ntrain: {'mIoU': 0.7336651786451073, 'F1': 0.8450460188223957, 'OA': 0.8377188578373244}\n{'ImSurf': 0.738216352672059, 'Building': 0.8447732263114057, 'LowVeg': 0.6871891898809406, 'Tree': 0.6771118000981314, 'Car': 0.7210353242629998, 'Clutter': 0.4078479186833989}\nval: {'mIoU': 0.7697430544493062, 'F1': 0.868681393977264, 'OA': 0.8634297291387707}\n{'ImSurf': 0.7771529649600484, 'Building': 0.8770195945499082, 'LowVeg': 0.7189297442436519, 'Tree': 0.7145705287797236, 'Car': 0.7610424397131992, 'Clutter': 0.49203452402205744}\nEpoch 10: loss: 215.7738, train accuracy: 0.8377, valid accuracy:0.8634\nStart epoch #11, learning rate for this epoch: [0.00048]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:34<00:00,  6.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #11, time for this epoch: 274.8666558265686s\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1786/1786 [01:11<00:00, 25.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"(array([0.7771792 , 0.8863131 , 0.72365607, 0.70994828, 0.75280598,\n       0.49241453]), 0.7699805241369859, 0.8686747608897308, 0.8639612420219706)\ntrain: {'mIoU': 0.7402537989843484, 'F1': 0.8494116977735257, 'OA': 0.8417173198625153}\n{'ImSurf': 0.7432878347529969, 'Building': 0.8519975598034734, 'LowVeg': 0.6938712895482426, 'Tree': 0.6823431601853736, 'Car': 0.7297691506316556, 'Clutter': 0.4122927614734381}\nval: {'mIoU': 0.7699805241369859, 'F1': 0.8686747608897308, 'OA': 0.8639612420219706}\n{'ImSurf': 0.7771792009223761, 'Building': 0.8863130966269696, 'LowVeg': 0.7236560666862738, 'Tree': 0.7099482784636308, 'Car': 0.7528059779856788, 'Clutter': 0.49241452776448336}\nEpoch 11: loss: 212.9324, train accuracy: 0.8417, valid accuracy:0.8640\nStart epoch #12, learning rate for this epoch: [0.000288]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:34<00:00,  6.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #12, time for this epoch: 274.1070122718811s\n","output_type":"stream"},{"name":"stderr","text":" 89%|████████▊ | 1585/1786 [01:01<00:07, 27.45it/s]Exception in thread Thread-20 (_pin_memory_loop):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 51, in _pin_memory_loop\n    do_one_step()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in do_one_step\n    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 307, in rebuild_storage_fd\n    fd = df.detach()\n  File \"/opt/conda/lib/python3.10/multiprocessing/resource_sharer.py\", line 58, in detach\n    return reduction.recv_handle(conn)\n  File \"/opt/conda/lib/python3.10/multiprocessing/reduction.py\", line 189, in recv_handle\n    return recvfds(s, 1)[0]\n  File \"/opt/conda/lib/python3.10/multiprocessing/reduction.py\", line 159, in recvfds\n    raise EOFError\nEOFError\n 89%|████████▉ | 1586/1786 [01:01<00:07, 25.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start epoch #13, learning rate for this epoch: [0.000288]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:34<00:00,  6.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #13, time for this epoch: 274.86873960494995s\n","output_type":"stream"},{"name":"stderr","text":" 91%|█████████ | 1617/1786 [01:01<00:06, 25.68it/s]Exception in thread Thread-21 (_pin_memory_loop):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 51, in _pin_memory_loop\n    do_one_step()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in do_one_step\n    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 307, in rebuild_storage_fd\n    fd = df.detach()\n  File \"/opt/conda/lib/python3.10/multiprocessing/resource_sharer.py\", line 58, in detach\n    return reduction.recv_handle(conn)\n  File \"/opt/conda/lib/python3.10/multiprocessing/reduction.py\", line 189, in recv_handle\n    return recvfds(s, 1)[0]\n  File \"/opt/conda/lib/python3.10/multiprocessing/reduction.py\", line 159, in recvfds\n    raise EOFError\nEOFError\n 91%|█████████ | 1620/1786 [01:06<00:06, 24.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start epoch #14, learning rate for this epoch: [0.000288]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:38<00:00,  6.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #14, time for this epoch: 278.0883574485779s\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1786/1786 [01:09<00:00, 25.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"(array([0.81255937, 0.91184124, 0.74690355, 0.74321714, 0.77669178,\n       0.55429505]), 0.7982426145734813, 0.8865202686350842, 0.8836842946693075)\ntrain: {'mIoU': 0.7635132615909561, 'F1': 0.8645353917308787, 'OA': 0.8589094081846605}\n{'ImSurf': 0.7714278500516847, 'Building': 0.8782552823877807, 'LowVeg': 0.7141515431924593, 'Tree': 0.7047727137730254, 'Car': 0.7489589185498305, 'Clutter': 0.4700585371746803}\nval: {'mIoU': 0.7982426145734813, 'F1': 0.8865202686350842, 'OA': 0.8836842946693075}\n{'ImSurf': 0.8125593705729312, 'Building': 0.9118412357077754, 'LowVeg': 0.7469035529343313, 'Tree': 0.7432171363600146, 'Car': 0.7766917772923542, 'Clutter': 0.5542950481265232}\nEpoch 14: loss: 197.7244, train accuracy: 0.8589, valid accuracy:0.8837\nStart epoch #15, learning rate for this epoch: [0.000288]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:34<00:00,  6.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #15, time for this epoch: 274.75355434417725s\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 863/1786 [00:32<00:35, 25.83it/s]Exception in thread Thread-23 (_pin_memory_loop):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 51, in _pin_memory_loop\n    do_one_step()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in do_one_step\n    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 307, in rebuild_storage_fd\n    fd = df.detach()\n  File \"/opt/conda/lib/python3.10/multiprocessing/resource_sharer.py\", line 58, in detach\n    return reduction.recv_handle(conn)\n  File \"/opt/conda/lib/python3.10/multiprocessing/reduction.py\", line 189, in recv_handle\n    return recvfds(s, 1)[0]\n  File \"/opt/conda/lib/python3.10/multiprocessing/reduction.py\", line 159, in recvfds\n    raise EOFError\nEOFError\n 48%|████▊     | 864/1786 [00:37<00:40, 22.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start epoch #16, learning rate for this epoch: [0.000288]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:35<00:00,  6.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #16, time for this epoch: 275.48927092552185s\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1786/1786 [01:08<00:00, 25.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"(array([0.81282694, 0.91630555, 0.74634041, 0.74713271, 0.78191453,\n       0.56548922]), 0.8009040289727338, 0.8881405451560609, 0.8853412701086546)\ntrain: {'mIoU': 0.7691308654582267, 'F1': 0.8680865045989922, 'OA': 0.8631603326450209}\n{'ImSurf': 0.7806693527925266, 'Building': 0.8855827199493183, 'LowVeg': 0.7174800373493293, 'Tree': 0.7092816043947255, 'Car': 0.7526406128052334, 'Clutter': 0.4828864021286697}\nval: {'mIoU': 0.8009040289727338, 'F1': 0.8881405451560609, 'OA': 0.8853412701086546}\n{'ImSurf': 0.8128269437519686, 'Building': 0.9163055480264367, 'LowVeg': 0.7463404085506512, 'Tree': 0.7471327104111463, 'Car': 0.7819145341234656, 'Clutter': 0.5654892195795875}\nEpoch 16: loss: 193.9590, train accuracy: 0.8632, valid accuracy:0.8853\nStart epoch #17, learning rate for this epoch: [0.000288]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:30<00:00,  6.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #17, time for this epoch: 270.9685437679291s\n","output_type":"stream"},{"name":"stderr","text":"  9%|▉         | 158/1786 [00:06<01:07, 24.05it/s]Exception in thread Thread-25 (_pin_memory_loop):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 51, in _pin_memory_loop\n    do_one_step()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in do_one_step\n    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 307, in rebuild_storage_fd\n    fd = df.detach()\n  File \"/opt/conda/lib/python3.10/multiprocessing/resource_sharer.py\", line 58, in detach\n    return reduction.recv_handle(conn)\n  File \"/opt/conda/lib/python3.10/multiprocessing/reduction.py\", line 189, in recv_handle\n    return recvfds(s, 1)[0]\n  File \"/opt/conda/lib/python3.10/multiprocessing/reduction.py\", line 159, in recvfds\n    raise EOFError\nEOFError\n  9%|▉         | 160/1786 [00:06<01:03, 25.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start epoch #18, learning rate for this epoch: [0.000288]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:33<00:00,  6.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #18, time for this epoch: 273.5017945766449s\n","output_type":"stream"},{"name":"stderr","text":" 24%|██▍       | 425/1786 [00:17<00:58, 23.32it/s]Exception in thread Thread-26 (_pin_memory_loop):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 51, in _pin_memory_loop\n    do_one_step()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in do_one_step\n    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 307, in rebuild_storage_fd\n    fd = df.detach()\n  File \"/opt/conda/lib/python3.10/multiprocessing/resource_sharer.py\", line 58, in detach\n    return reduction.recv_handle(conn)\n  File \"/opt/conda/lib/python3.10/multiprocessing/reduction.py\", line 189, in recv_handle\n    return recvfds(s, 1)[0]\n  File \"/opt/conda/lib/python3.10/multiprocessing/reduction.py\", line 159, in recvfds\n    raise EOFError\nEOFError\n 24%|██▍       | 427/1786 [00:17<00:55, 24.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start epoch #19, learning rate for this epoch: [0.000288]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:34<00:00,  6.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #19, time for this epoch: 274.9062123298645s\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1786/1786 [01:10<00:00, 25.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"(array([0.8383986 , 0.92530469, 0.7677219 , 0.75831159, 0.79705835,\n       0.57961622]), 0.8173590282069055, 0.8983031222198802, 0.8955442539026263)\ntrain: {'mIoU': 0.7768868832351024, 'F1': 0.8731090679231579, 'OA': 0.8696876686160304}\n{'ImSurf': 0.789991249673485, 'Building': 0.8904653782144216, 'LowVeg': 0.7300194972693762, 'Tree': 0.7176479575318359, 'Car': 0.7563103334863926, 'Clutter': 0.507344643199719}\nval: {'mIoU': 0.8173590282069055, 'F1': 0.8983031222198802, 'OA': 0.8955442539026263}\n{'ImSurf': 0.8383986031895411, 'Building': 0.9253046939206796, 'LowVeg': 0.7677219026147167, 'Tree': 0.7583115896779447, 'Car': 0.7970583516316452, 'Clutter': 0.5796162240091093}\nEpoch 19: loss: 188.5116, train accuracy: 0.8697, valid accuracy:0.8955\nStart epoch #20, learning rate for this epoch: [0.000288]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:34<00:00,  6.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #20, time for this epoch: 274.9883089065552s\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1786/1786 [01:08<00:00, 26.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"(array([0.83620305, 0.92370827, 0.75491902, 0.69732321, 0.79991272,\n       0.5888944 ]), 0.8024132539377933, 0.8883984970686957, 0.8885094564451621)\ntrain: {'mIoU': 0.7815072096647895, 'F1': 0.8760381324031684, 'OA': 0.8729334150041853}\n{'ImSurf': 0.7973456012138692, 'Building': 0.894190281021065, 'LowVeg': 0.7336781652818678, 'Tree': 0.7225395389933685, 'Car': 0.7597824618137771, 'Clutter': 0.5155182596492743}\nval: {'mIoU': 0.8024132539377933, 'F1': 0.8883984970686957, 'OA': 0.8885094564451621}\n{'ImSurf': 0.8362030481301057, 'Building': 0.923708265469095, 'LowVeg': 0.7549190246525108, 'Tree': 0.6973232106596009, 'Car': 0.7999127207776541, 'Clutter': 0.588894402784398}\nEpoch 20: loss: 186.0955, train accuracy: 0.8729, valid accuracy:0.8885\nStart epoch #21, learning rate for this epoch: [0.0001728]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:38<00:00,  6.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #21, time for this epoch: 278.3944413661957s\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1786/1786 [01:08<00:00, 25.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"(array([0.85566384, 0.93687806, 0.78623187, 0.77025915, 0.81393998,\n       0.62623644]), 0.8325945798023309, 0.907520637804954, 0.9063546439057796)\ntrain: {'mIoU': 0.7915624728543177, 'F1': 0.8823575389458791, 'OA': 0.880038061061827}\n{'ImSurf': 0.80888067246605, 'Building': 0.9038247122521507, 'LowVeg': 0.7457354320609211, 'Tree': 0.7308294715702687, 'Car': 0.7685420759221981, 'Clutter': 0.5373466741659053}\nval: {'mIoU': 0.8325945798023309, 'F1': 0.907520637804954, 'OA': 0.9063546439057796}\n{'ImSurf': 0.8556638448373617, 'Building': 0.9368780579447078, 'LowVeg': 0.7862318678321532, 'Tree': 0.7702591482665625, 'Car': 0.8139399801308693, 'Clutter': 0.6262364354192532}\nEpoch 21: loss: 179.9831, train accuracy: 0.8800, valid accuracy:0.9064\nStart epoch #22, learning rate for this epoch: [0.0001728]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:36<00:00,  6.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #22, time for this epoch: 276.7336368560791s\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1786/1786 [01:09<00:00, 25.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"(array([0.78386633, 0.92137781, 0.73436286, 0.72136442, 0.80477352,\n       0.60677495]), 0.7931489871399151, 0.8829435056554399, 0.8777681599225533)\ntrain: {'mIoU': 0.793952331582995, 'F1': 0.883857756638674, 'OA': 0.8824454128575259}\n{'ImSurf': 0.8144667526678392, 'Building': 0.9052412145822293, 'LowVeg': 0.7480864384667013, 'Tree': 0.735239915625608, 'Car': 0.7667273365725973, 'Clutter': 0.5409262318219472}\nval: {'mIoU': 0.7931489871399151, 'F1': 0.8829435056554399, 'OA': 0.8777681599225533}\n{'ImSurf': 0.7838663302410728, 'Building': 0.9213778104288045, 'LowVeg': 0.7343628554373318, 'Tree': 0.7213644158156041, 'Car': 0.8047735237767621, 'Clutter': 0.6067749487163149}\nEpoch 22: loss: 178.3413, train accuracy: 0.8824, valid accuracy:0.8778\nStart epoch #23, learning rate for this epoch: [0.0001728]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:34<00:00,  6.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #23, time for this epoch: 274.9800503253937s\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1786/1786 [01:10<00:00, 25.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"(array([0.8547282 , 0.93242003, 0.79088023, 0.77572259, 0.81718607,\n       0.64162798]), 0.8341874227947847, 0.9086058253136375, 0.907828142101638)\ntrain: {'mIoU': 0.7962129715689741, 'F1': 0.8853136607151317, 'OA': 0.8835606230407201}\n{'ImSurf': 0.815649647999906, 'Building': 0.9052354998476483, 'LowVeg': 0.7508549707901694, 'Tree': 0.7374770741954951, 'Car': 0.7718476650116518, 'Clutter': 0.5513167126675064}\nval: {'mIoU': 0.8341874227947847, 'F1': 0.9086058253136375, 'OA': 0.907828142101638}\n{'ImSurf': 0.854728195935228, 'Building': 0.9324200259501334, 'LowVeg': 0.7908802328740326, 'Tree': 0.7757225903163334, 'Car': 0.8171860688981968, 'Clutter': 0.6416279819462759}\nEpoch 23: loss: 176.8945, train accuracy: 0.8836, valid accuracy:0.9078\nStart epoch #24, learning rate for this epoch: [0.0001728]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:34<00:00,  6.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #24, time for this epoch: 274.0754771232605s\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1786/1786 [01:07<00:00, 26.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"(array([0.86084165, 0.94044206, 0.7909469 , 0.77956462, 0.81461629,\n       0.64109085]), 0.8372823029376579, 0.9103530136274035, 0.9098842860954128)\ntrain: {'mIoU': 0.7994894869076372, 'F1': 0.8873715762847073, 'OA': 0.8857667116223931}\n{'ImSurf': 0.8198502708602281, 'Building': 0.9068447955242497, 'LowVeg': 0.7555959541890718, 'Tree': 0.7401283620803965, 'Car': 0.77502805188424, 'Clutter': 0.5622137054797453}\nval: {'mIoU': 0.8372823029376579, 'F1': 0.9103530136274035, 'OA': 0.9098842860954128}\n{'ImSurf': 0.8608416523625688, 'Building': 0.9404420570012431, 'LowVeg': 0.7909469034833378, 'Tree': 0.7795646153931205, 'Car': 0.81461628644802, 'Clutter': 0.6410908459691866}\nEpoch 24: loss: 174.6396, train accuracy: 0.8858, valid accuracy:0.9099\nStart epoch #25, learning rate for this epoch: [0.00010368]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:35<00:00,  6.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #25, time for this epoch: 275.44224643707275s\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1786/1786 [01:08<00:00, 26.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"(array([0.85152685, 0.94012699, 0.78576824, 0.77670879, 0.82632145,\n       0.65781226]), 0.8360904647377861, 0.909641912985153, 0.9083050040982396)\ntrain: {'mIoU': 0.8057286738160488, 'F1': 0.8912441914171414, 'OA': 0.8898877034334242}\n{'ImSurf': 0.8263052557327849, 'Building': 0.911875047381227, 'LowVeg': 0.7626787007392369, 'Tree': 0.7461537323280537, 'Car': 0.7816306328989413, 'Clutter': 0.5773976178079925}\nval: {'mIoU': 0.8360904647377861, 'F1': 0.909641912985153, 'OA': 0.9083050040982396}\n{'ImSurf': 0.8515268542443085, 'Building': 0.9401269867075043, 'LowVeg': 0.7857682424420426, 'Tree': 0.7767087948032813, 'Car': 0.8263214454917945, 'Clutter': 0.657812260886745}\nEpoch 25: loss: 171.9145, train accuracy: 0.8899, valid accuracy:0.9083\nStart epoch #26, learning rate for this epoch: [0.00010368]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:37<00:00,  6.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #26, time for this epoch: 277.4007785320282s\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1786/1786 [01:10<00:00, 25.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"(array([0.86622485, 0.94372893, 0.79517555, 0.78722314, 0.82677868,\n       0.66295026]), 0.8438262305636435, 0.9142785450288873, 0.9136592696997196)\ntrain: {'mIoU': 0.8082950927838082, 'F1': 0.8928028281486219, 'OA': 0.8921734101965981}\n{'ImSurf': 0.8313174676217066, 'Building': 0.914220935139945, 'LowVeg': 0.7654241439390044, 'Tree': 0.7473759801254414, 'Car': 0.7831369370929444, 'Clutter': 0.5849494410564202}\nval: {'mIoU': 0.8438262305636435, 'F1': 0.9142785450288873, 'OA': 0.9136592696997196}\n{'ImSurf': 0.8662248505044983, 'Building': 0.9437289312899806, 'LowVeg': 0.795175551110603, 'Tree': 0.7872231357825802, 'Car': 0.8267786841305558, 'Clutter': 0.6629502599650923}\nEpoch 26: loss: 170.2574, train accuracy: 0.8922, valid accuracy:0.9137\nStart epoch #27, learning rate for this epoch: [0.00010368]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:37<00:00,  6.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #27, time for this epoch: 277.73217606544495s\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1786/1786 [01:09<00:00, 25.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"(array([0.87280447, 0.94359307, 0.80557876, 0.78407439, 0.82768522,\n       0.65875172]), 0.8467471822969035, 0.9160145947089834, 0.9158045146375663)\ntrain: {'mIoU': 0.8101016671341034, 'F1': 0.8938657407097838, 'OA': 0.8929503058852936}\n{'ImSurf': 0.8332149939796811, 'Building': 0.9173426057044046, 'LowVeg': 0.7654518273682722, 'Tree': 0.7461517568949748, 'Car': 0.7883471517231844, 'Clutter': 0.5837642228856629}\nval: {'mIoU': 0.8467471822969035, 'F1': 0.9160145947089834, 'OA': 0.9158045146375663}\n{'ImSurf': 0.8728044713329972, 'Building': 0.9435930737899464, 'LowVeg': 0.8055787588391394, 'Tree': 0.784074392228916, 'Car': 0.8276852152935182, 'Clutter': 0.6587517196231988}\nEpoch 27: loss: 169.3184, train accuracy: 0.8930, valid accuracy:0.9158\nStart epoch #28, learning rate for this epoch: [0.00010368]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1785/1785 [04:34<00:00,  6.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done epoch #28, time for this epoch: 274.4540774822235s\n","output_type":"stream"},{"name":"stderr","text":" 42%|████▏     | 750/1786 [00:28<00:42, 24.61it/s]Exception in thread Thread-36 (_pin_memory_loop):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 51, in _pin_memory_loop\n    do_one_step()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in do_one_step\n    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 307, in rebuild_storage_fd\n    fd = df.detach()\n  File \"/opt/conda/lib/python3.10/multiprocessing/resource_sharer.py\", line 58, in detach\n    return reduction.recv_handle(conn)\n  File \"/opt/conda/lib/python3.10/multiprocessing/reduction.py\", line 189, in recv_handle\n    return recvfds(s, 1)[0]\n  File \"/opt/conda/lib/python3.10/multiprocessing/reduction.py\", line 159, in recvfds\n    raise EOFError\nEOFError\n 42%|████▏     | 751/1786 [00:33<00:46, 22.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start epoch #29, learning rate for this epoch: [0.00010368]\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 182/1785 [00:28<03:47,  7.05it/s]","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"papermill":{"duration":0.03199,"end_time":"2023-11-13T09:21:20.171938","exception":false,"start_time":"2023-11-13T09:21:20.139948","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.024586,"end_time":"2023-11-13T09:21:20.221336","exception":false,"start_time":"2023-11-13T09:21:20.19675","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot the learning cure","metadata":{"papermill":{"duration":0.024534,"end_time":"2023-11-13T09:21:20.270588","exception":false,"start_time":"2023-11-13T09:21:20.246054","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# load_model(model, checkpoint)","metadata":{"papermill":{"duration":0.031504,"end_time":"2023-11-13T09:21:20.326709","exception":false,"start_time":"2023-11-13T09:21:20.295205","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.dpi'] = 90\nplt.rcParams['figure.figsize'] = (6, 4)\nepochs_array = range(epochs)","metadata":{"papermill":{"duration":0.031761,"end_time":"2023-11-13T09:21:20.383049","exception":false,"start_time":"2023-11-13T09:21:20.351288","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Training and Test loss\nplt.plot(epochs_array, train_loss_array, 'g', label='Training loss')\nplt.plot(epochs_array, test_loss_array, 'b', label='Test loss')\nplt.title('Training and Test loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"papermill":{"duration":0.324591,"end_time":"2023-11-13T09:21:20.732601","exception":false,"start_time":"2023-11-13T09:21:20.40801","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Infer**","metadata":{"papermill":{"duration":0.025171,"end_time":"2023-11-13T09:21:20.78381","exception":false,"start_time":"2023-11-13T09:21:20.758639","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# from torch.jit import load\n# model = UNet()\n# optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n\n# checkpoint = torch.load(pretrained_path)","metadata":{"papermill":{"duration":0.032171,"end_time":"2023-11-13T09:21:20.841338","exception":false,"start_time":"2023-11-13T09:21:20.809167","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer.load_state_dict(checkpoint['optimizer'])","metadata":{"papermill":{"duration":0.031681,"end_time":"2023-11-13T09:21:20.898418","exception":false,"start_time":"2023-11-13T09:21:20.866737","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from collections import OrderedDict\n# new_state_dict = OrderedDict()\n# for k, v in checkpoint['model'].items():\n#     name = k[7:] # remove `module.`\n#     new_state_dict[name] = v\n# # load params\n# model.load_state_dict(new_state_dict)","metadata":{"papermill":{"duration":0.032386,"end_time":"2023-11-13T09:21:20.956384","exception":false,"start_time":"2023-11-13T09:21:20.923998","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.025216,"end_time":"2023-11-13T09:21:21.007264","exception":false,"start_time":"2023-11-13T09:21:20.982048","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualize results**","metadata":{"papermill":{"duration":0.025228,"end_time":"2023-11-13T09:21:21.058135","exception":false,"start_time":"2023-11-13T09:21:21.032907","status":"completed"},"tags":[]}},{"cell_type":"code","source":"for i, (data, label) in enumerate(train_dataloader):\n    img = data\n    mask = label\n    break","metadata":{"papermill":{"duration":0.156265,"end_time":"2023-11-13T09:21:21.239924","exception":false,"start_time":"2023-11-13T09:21:21.083659","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, arr = plt.subplots(4, 3, figsize=(16, 12))\n# arr[0][0].set_title('Image')\n# arr[0][1].set_title('Segmentation')\n# arr[0][2].set_title('Predict')\n\n# model.eval()\n# with torch.no_grad():\n#     z = img.to(device)\n#     predict = model(z)\n\n# def display_image_grid(images_filenames, images_directory, masks_directory, predicted_masks=None):\n#     cols = 3 if predicted_masks else 2\n#     rows = len(images_filenames)\n#     figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(10, 24))\n#     for i, image_filename in enumerate(images_filenames):\n#         image = cv2.imread(os.path.join(images_directory, image_filename))\n#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n#         mask = cv2.imread(os.path.join(masks_directory, image_filename.replace(\".jpeg\", \".png\")), cv2.IMREAD_UNCHANGED,)\n#         mask = preprocess_mask(mask)\n#         ax[i, 0].imshow(image)\n#         ax[i, 1].imshow(mask, interpolation=\"nearest\")\n\n#         ax[i, 0].set_title(\"Image\")\n#         ax[i, 1].set_title(\"Ground truth mask\")\n\n#         ax[i, 0].set_axis_off()\n#         ax[i, 1].set_axis_off()\n\n#         if predicted_masks:\n#             predicted_mask = predicted_masks[i]\n#             ax[i, 2].imshow(predicted_mask, interpolation=\"nearest\")\n#             ax[i, 2].set_title(\"Predicted mask\")\n#             ax[i, 2].set_axis_off()\n#     plt.tight_layout()\n#     plt.show()\n\n# display_image_grid(, images_directory, masks_directory,predicted_masks = predict)    \n# # for i in range(2):\n# #     arr[i][0].imshow(img[i].permute(1, 2, 0));\n    \n# #     arr[i][1].imshow(F.one_hot(mask[i]).float())\n    \n# #     arr[i][2].imshow(F.one_hot(torch.argmax(predict[i], 0).cpu()).float())","metadata":{"papermill":{"duration":3.94406,"end_time":"2023-11-13T09:21:25.210255","exception":true,"start_time":"2023-11-13T09:21:21.266195","status":"failed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create submission**","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def img_writer(inp):\n    (mask,  mask_id, rgb) = inp\n    if rgb:\n        mask_name_tif = mask_id + '.png'\n        mask_tif = label2rgb(mask)\n        cv2.imwrite(mask_name_tif, mask_tif)\n    else:\n        mask_png = mask.astype(np.uint8)\n        mask_name_png = mask_id + '.png'\n        cv2.imwrite(mask_name_png, mask_png)\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    arg = parser.add_argument\n    arg(\"-c\", \"--config_path\", type=Path, required=True, help=\"Path to  config\")\n    arg(\"-o\", \"--output_path\", type=Path, help=\"Path where to save resulting masks.\", required=True)\n    arg(\"-t\", \"--tta\", help=\"Test time augmentation.\", default=None, choices=[None, \"d4\", \"lr\"])\n    arg(\"--rgb\", help=\"whether output rgb images\", action='store_true')\n    return parser.parse_args()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = get_args()\nseed_everything(42)\n\nconfig = py2cfg(args.config_path)\nargs.output_path.mkdir(exist_ok=True, parents=True)\n\nmodel = Supervision_Train.load_from_checkpoint(\n    os.path.join(config.weights_path, config.test_weights_name + '.ckpt'), config=config)\nmodel.cuda()\nmodel.eval()\nevaluator = Evaluator(num_class=config.num_classes)\nevaluator.reset()\nif args.tta == \"lr\":\n    transforms = tta.Compose(\n        [\n            tta.HorizontalFlip(),\n            tta.VerticalFlip()\n        ]\n    )\n    model = tta.SegmentationTTAWrapper(model, transforms)\nelif args.tta == \"d4\":\n    transforms = tta.Compose(\n        [\n            tta.HorizontalFlip(),\n            tta.VerticalFlip(),\n            # tta.Rotate90(angles=[90]),\n            tta.Scale(scales=[0.75, 1.0, 1.25, 1.5], interpolation='bicubic', align_corners=False)\n        ]\n    )\n    model = tta.SegmentationTTAWrapper(model, transforms)\n\ntest_dataset = config.test_dataset\n\nwith torch.no_grad():\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=2,\n        num_workers=4,\n        pin_memory=True,\n        drop_last=False,\n    )\n    results = []\n    for input in tqdm(test_loader):\n        # raw_prediction NxCxHxW\n        raw_predictions, _ = model(input['img'].cuda())\n\n        image_ids = input[\"img_id\"]\n        masks_true = input['gt_semantic_seg']\n\n        raw_predictions, _ = nn.Softmax(dim=1)(raw_predictions)\n        predictions = raw_predictions.argmax(dim=1)\n\n        for i in range(raw_predictions.shape[0]):\n            mask = predictions[i].cpu().numpy()\n            evaluator.add_batch(pre_image=mask, gt_image=masks_true[i].cpu().numpy())\n            mask_name = image_ids[i]\n            results.append((mask, str(args.output_path / mask_name), args.rgb))\niou_per_class = evaluator.Intersection_over_Union()\nf1_per_class = evaluator.F1()\nOA = evaluator.OA()\nfor class_name, class_iou, class_f1 in zip(CLASSES, iou_per_class, f1_per_class):\n    print('F1_{}:{}, IOU_{}:{}'.format(class_name, class_f1, class_name, class_iou))\nprint('F1:{}, mIOU:{}, OA:{}'.format(np.nanmean(f1_per_class[:-1]), np.nanmean(iou_per_class[:-1]), OA))\nt0 = time.time()\nmpp.Pool(processes=mp.cpu_count()).map(img_writer, results)\nt1 = time.time()\nimg_write_time = t1 - t0\nprint('images writing spends: {} s'.format(img_write_time))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = A.Compose([A.Resize(384, 384),\n                       A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n                     ToTensorV2(),])","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, arr = plt.subplots(5, 2, figsize=(16, 12))\n# arr[0][0].set_title('Image');\n# arr[0][1].set_title('Predict');\n\n# model.eval()\n# with torch.no_grad():\n#     z = img.to(device)\n#     predict = model(z)\n\n# for i in range(5):\n#     arr[i][0].imshow(img[i].permute(1, 2, 0));\n#     arr[i][1].imshow(F.one_hot(torch.argmax(predict[i], 0).cpu()).float())","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nif not os.path.isdir(\"/kaggle/working/predicted_masks\"):\n    os.mkdir(\"/kaggle/working/predicted_masks\")\nfor _, (img, path, H, W) in enumerate(test_dataloader):\n    a = path\n    b = img\n    h = H\n    w = W\n    \n    with torch.no_grad():\n        z = b.to(device)\n        predicted_mask = model(z)\n    for i in range(len(a)):\n        image_id = a[i].split('/')[-1].split('.')[0]\n        filename = image_id + \".png\"\n        mask2img = Resize((h[i].item(), w[i].item()), interpolation=InterpolationMode.NEAREST)(ToPILImage()(F.one_hot(torch.argmax(predicted_mask[i], 0)).permute(2, 0, 1).float()))\n        mask2img.save(os.path.join(\"/kaggle/working/predicted_masks/\", filename))","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rle_to_string(runs):\n    return ' '.join(str(x) for x in runs)\n\ndef rle_encode_one_mask(mask):\n    pixels = mask.flatten()\n    pixels[pixels > 0] = 255\n    use_padding = False\n    if pixels[0] or pixels[-1]:\n        use_padding = True\n        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n        pixel_padded[1:-1] = pixels\n        pixels = pixel_padded\n    \n    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    if use_padding:\n        rle = rle - 1\n    rle[1::2] = rle[1::2] - rle[:-1:2]\n    return rle_to_string(rle)\n\ndef mask2string(dir):\n    ## mask --> string\n    strings = []\n    ids = []\n    ws, hs = [[] for i in range(2)]\n    for image_id in os.listdir(dir):\n        id = image_id.split('.')[0]\n        path = os.path.join(dir, image_id)\n        print(path)\n        img = cv2.imread(path)[:,:,::-1]\n        h, w = img.shape[0], img.shape[1]\n        for channel in range(2):\n            ws.append(w)\n            hs.append(h)\n            ids.append(f'{id}_{channel}')\n            string = rle_encode_one_mask(img[:,:,channel])\n            strings.append(string)\n    r = {\n        'ids': ids,\n        'strings': strings,\n    }\n    return r\n\n\nMASK_DIR_PATH = '/kaggle/working/predicted_masks' # change this to the path to your output mask folder\ndir = MASK_DIR_PATH\nres = mask2string(dir)\ndf = pd.DataFrame(columns=['Id', 'Expected'])\ndf['Id'] = res['ids']\ndf['Expected'] = res['strings']\ndf.to_csv(r'output.csv', index=False)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}